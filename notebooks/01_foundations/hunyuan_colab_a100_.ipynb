{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# HunyuanVideo on Google Colab A100\n",
        "\n",
        "**Generate high-quality videos using Tencent's HunyuanVideo model on Google Colab A100 GPU**\n",
        "\n",
        "üöÄ **What you'll learn:**\n",
        "- Run the 13B parameter HunyuanVideo model on Colab A100 (40GB)\n",
        "- Optimize memory usage for large-scale video generation\n",
        "- Generate videos up to 15 seconds with advanced prompting\n",
        "- Export and download high-quality video results\n",
        "\n",
        "‚ö° **Requirements:**\n",
        "- Google Colab Pro+ with A100 GPU access\n",
        "- ~20-30 minutes for complete setup\n",
        "- Google Drive for video storage (optional)\n",
        "\n",
        "üìö **From the Book:** *Hands-On Video Generation with AI* - Chapter 3: Advanced Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## üîß 1. Environment Setup & GPU Verification\n",
        "\n",
        "First, let's verify we have an A100 GPU and configure the environment for optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ 1. Install Dependencies\n",
        "\n",
        "Install the latest versions of required libraries optimized for HunyuanVideo."
      ],
      "metadata": {
        "id": "iT3aX9Ox3mHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-diffusers",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa7d434-d0aa-44c9-e668-41dd8da1c968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.11/dist-packages (2.4.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.19.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.4.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.27.post2+cu118)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: diffusers[torch] in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.8.86)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (8.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (0.32.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (2.32.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers[torch]) (1.1.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers[torch]) (3.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers[torch]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers[torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers[torch]) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        " # Install HunyuanVideo dependencies\n",
        "!pip install torch==2.4.0 torchvision torchaudio \\\n",
        "             diffusers[torch] \\\n",
        "             transformers \\\n",
        "             accelerate \\\n",
        "             safetensors \\\n",
        "             xformers \\\n",
        "             imageio-ffmpeg --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix transformers"
      ],
      "metadata": {
        "id": "UmeTEfvKgLl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import modeling_utils\n",
        "print(\"Available parallel styles:\", modeling_utils.ALL_PARALLEL_STYLES)\n",
        "if not hasattr(modeling_utils, \"ALL_PARALLEL_STYLES\") or modeling_utils.ALL_PARALLEL_STYLES is None:\n",
        "    modeling_utils.ALL_PARALLEL_STYLES = [\"tp\", \"none\",\"colwise\",'rowwise']\n",
        "    print(\"!!Fixed!! available parallel styles:\", modeling_utils.ALL_PARALLEL_STYLES)"
      ],
      "metadata": {
        "id": "Kukez3SDgK0S",
        "outputId": "412ac52f-d7fc-44fe-fad7-e0939c177e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available parallel styles: None\n",
            "!!Fixed!! available parallel styles: ['tp', 'none', 'colwise', 'rowwise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9181f0b9-6754-409c-f438-5c7fa78aa019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun  8 23:38:39 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             47W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "üñ•Ô∏è GPU Information:\n",
            "   GPU: NVIDIA A100-SXM4-40GB\n",
            "   Memory: 40,960 MB (40.0 GB)\n",
            "   ‚úÖ Perfect! A100 40GB detected - optimal for HunyuanVideo\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability and specifications\n",
        "!nvidia-smi\n",
        "\n",
        "# Verify we have A100 access\n",
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'],\n",
        "                       capture_output=True, text=True)\n",
        "print(\"\\nüñ•Ô∏è GPU Information:\")\n",
        "gpu_info = result.stdout.strip().split(', ')\n",
        "if len(gpu_info) >= 2:\n",
        "    gpu_name, gpu_memory = gpu_info[0], int(gpu_info[1])\n",
        "    print(f\"   GPU: {gpu_name}\")\n",
        "    print(f\"   Memory: {gpu_memory:,} MB ({gpu_memory/1024:.1f} GB)\")\n",
        "\n",
        "    if \"A100\" in gpu_name and gpu_memory >= 40000:\n",
        "        print(\"   ‚úÖ Perfect! A100 40GB detected - optimal for HunyuanVideo\")\n",
        "    elif \"A100\" in gpu_name:\n",
        "        print(\"   ‚ö†Ô∏è A100 detected but check memory - may need optimization\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Warning: A100 GPU recommended for best performance\")\n",
        "        print(\"   üí° Consider upgrading to Colab Pro+ for A100 access\")\n",
        "else:\n",
        "    print(\"   ‚ùå Unable to detect GPU information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env-config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1402dd-f577-4a40-83a1-1a50bc8cee3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Environment configured for A100 optimization\n",
            "   PyTorch version: 2.4.0+cu118\n",
            "   CUDA available: True\n",
            "   CUDA version: 11.8\n",
            "   GPU count: 1\n"
          ]
        }
      ],
      "source": [
        "# Configure environment for maximum memory efficiency\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Essential memory optimizations for A100 40GB\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # Async for better performance\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid warnings\n",
        "\n",
        "# Enable optimized math operations\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "print(\"üîß Environment configured for A100 optimization\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"   GPU count: {torch.cuda.device_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-header"
      },
      "source": [
        "## üì¶ 2. Install Book package\n",
        "\n",
        "Install the latest version of the book library package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-videogenbook"
      },
      "outputs": [],
      "source": [
        "# Install the videogenbook package\n",
        "!pip install git+https://github.com/jenochs/video-generation-book.git\n",
        "\n",
        "# Verify installation\n",
        "import videogenbook\n",
        "print(f\"‚úÖ videogenbook v{videogenbook.__version__} installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memory-header"
      },
      "source": [
        "## üß† 3. Memory Monitoring & Optimization\n",
        "\n",
        "Set up memory monitoring and configure HunyuanVideo for A100 40GB constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-utils",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfc0382-4ca5-4798-9dfe-6d35ab1e018d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† GPU Memory (Initial):\n",
            "   Total: 39.6 GB\n",
            "   Allocated: 0.0 GB\n",
            "   Cached: 0.0 GB\n",
            "   Free: 39.6 GB\n",
            "   üöÄ Excellent memory - can use higher quality settings\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "from typing import Dict, Any\n",
        "\n",
        "def get_gpu_memory() -> Dict[str, float]:\n",
        "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return {\"total\": 0, \"used\": 0, \"free\": 0}\n",
        "\n",
        "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    cached = torch.cuda.memory_reserved() / 1024**3\n",
        "    free = total - cached\n",
        "\n",
        "    return {\n",
        "        \"total\": total,\n",
        "        \"allocated\": allocated,\n",
        "        \"cached\": cached,\n",
        "        \"free\": free\n",
        "    }\n",
        "\n",
        "def print_memory_status(stage: str = \"\"):\n",
        "    \"\"\"Print current memory status.\"\"\"\n",
        "    mem = get_gpu_memory()\n",
        "    print(f\"üß† GPU Memory {stage}:\")\n",
        "    print(f\"   Total: {mem['total']:.1f} GB\")\n",
        "    print(f\"   Allocated: {mem['allocated']:.1f} GB\")\n",
        "    print(f\"   Cached: {mem['cached']:.1f} GB\")\n",
        "    print(f\"   Free: {mem['free']:.1f} GB\")\n",
        "\n",
        "    # Memory warnings\n",
        "    if mem['free'] < 10:\n",
        "        print(\"   ‚ö†Ô∏è Low memory - consider reducing resolution/frames\")\n",
        "    elif mem['free'] < 20:\n",
        "        print(\"   ‚úÖ Sufficient memory for standard generation\")\n",
        "    else:\n",
        "        print(\"   üöÄ Excellent memory - can use higher quality settings\")\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Cleanup GPU memory.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "# Initial memory check\n",
        "print_memory_status(\"(Initial)\")\n",
        "cleanup_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-header"
      },
      "source": [
        "## ü§ñ 4. Load HunyuanVideo Model\n",
        "\n",
        "Load the HunyuanVideo model with A100-optimized settings for 40GB memory constraint."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import HunyuanVideoPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "\n",
        "\n",
        "pipe = HunyuanVideoPipeline.from_pretrained(\n",
        "    \"hunyuanvideo-community/HunyuanVideo\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n"
      ],
      "metadata": {
        "id": "8GRL-v-LZmbT",
        "outputId": "f85fff51-288a-4736-e540-8a447fc18715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "e647f78828394bada20a973765802617",
            "92070acddab4441186ea839bcac86e5f",
            "6d3d3c02d4fb4cc88a9dcbf59ab41b2d",
            "63150f18d5b94a9691e39bf4325b8d2f",
            "b8d38c80fe4f4bd98ea89af12988e29a",
            "621f031f5691495b9bbfbdcadcbf3626",
            "9fb99b56e2c441d1b5f0c96f552a4188",
            "a40997681347407c9bfa0e47cd2de4e3",
            "db9e5ce6b76448c68b9672d09853ef21",
            "38035ea5994b4e708a4d14a679bcc0eb",
            "a77c3a0365c04cb78c0e6d715df444fc",
            "dbae256f1a89414fbd5fd34277cd1dae",
            "1931ef55e2804a3786672074445dea96",
            "3d9a9e30412e43df85b8934ebe1d3b25",
            "acf92a9603c645f09c28a637beb48eb9",
            "c8cb034ee86c4ad39ee723ec70a083dc",
            "ab52539a99d04d09adb9923898ec61b6",
            "981a220ac09e47a39a987e3b58042c99",
            "cb1aeb42fea04e39be432975a1cf62c6",
            "dcd5bb0d262a4c059ebadad6ab99ae84",
            "48e1606f81d54af99132275bc8027672",
            "ea165d4c03364d2084063ad6cc747f52",
            "45b986ddc2a445f3bab05b18fe6f147a",
            "7d5919838f2d4aebbcf1987a8eff045f",
            "1e7f9c32d1f64e6fabd46e87b5668b5c",
            "fd29bc0f9f91436c866d8f33b3037fe4",
            "51f79834e4634beaab90d6773d82febb",
            "2a53410330e640bc9c4cb1b3b225745c",
            "05310a718fab4619940e4ff8c99ca2c2",
            "9af9dc1481b640d2983ae5dfc43150a6",
            "0b4caf3065cd44f796ac4c6cc3de509c",
            "5ba5f136e4284aefb777154a019831bd",
            "d7840f70f78749a0b719b0dc60d23743"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
            "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e647f78828394bada20a973765802617"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbae256f1a89414fbd5fd34277cd1dae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45b986ddc2a445f3bab05b18fe6f147a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "## ‚öôÔ∏è 5. A100-Optimized Generation Settings\n",
        "\n",
        "Configure generation parameters optimized for A100 40GB memory constraints while maintaining high quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config-settings",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8958886-06d4-4bd8-91af-5ce2097cd45c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Available Quality Configurations:\n",
            "\n",
            "üé• HIGH_QUALITY:\n",
            "   Resolution: 1280x720\n",
            "   Duration: ~8.1s (65 frames)\n",
            "   Steps: 30\n",
            "   üí° High quality 720p (may use significant memory)\n",
            "\n",
            "üé• BALANCED:\n",
            "   Resolution: 960x544\n",
            "   Duration: ~8.1s (65 frames)\n",
            "   Steps: 25\n",
            "   üí° Balanced quality and memory usage (recommended)\n",
            "\n",
            "üé• FAST:\n",
            "   Resolution: 512x512\n",
            "   Duration: ~4.0s (32 frames)\n",
            "   Steps: 20\n",
            "   üí° Fast generation with lower memory usage\n",
            "\n",
            "üíæ Estimated Memory Usage:\n",
            "   High Quality: ~35-38GB (may require cleanup between generations)\n",
            "   Balanced: ~25-30GB (recommended for most use cases)\n",
            "   Fast: ~15-20GB (reliable for multiple generations)\n"
          ]
        }
      ],
      "source": [
        "# A100 40GB optimized configurations\n",
        "COLAB_CONFIGS = {\n",
        "    \"high_quality\": {\n",
        "        \"height\": 720,\n",
        "        \"width\": 1280,\n",
        "        \"num_frames\": 65,           # Reduced from 129 for memory\n",
        "        \"guidance_scale\": 7.0,\n",
        "        \"num_inference_steps\": 30,  # Balanced quality/speed\n",
        "        \"description\": \"High quality 720p (may use significant memory)\"\n",
        "    },\n",
        "    \"balanced\": {\n",
        "        \"height\": 544,\n",
        "        \"width\": 960,\n",
        "        \"num_frames\": 65,\n",
        "        \"guidance_scale\": 7.0,\n",
        "        \"num_inference_steps\": 25,\n",
        "        \"description\": \"Balanced quality and memory usage (recommended)\"\n",
        "    },\n",
        "    \"fast\": {\n",
        "        \"height\": 512,\n",
        "        \"width\": 512,\n",
        "        \"num_frames\": 32,\n",
        "        \"guidance_scale\": 6.0,\n",
        "        \"num_inference_steps\": 20,\n",
        "        \"description\": \"Fast generation with lower memory usage\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def print_config_options():\n",
        "    \"\"\"Display available configuration options.\"\"\"\n",
        "    print(\"üìä Available Quality Configurations:\")\n",
        "    for name, config in COLAB_CONFIGS.items():\n",
        "        duration = config['num_frames'] / 8  # Assuming 8 FPS\n",
        "        print(f\"\\nüé• {name.upper()}:\")\n",
        "        print(f\"   Resolution: {config['width']}x{config['height']}\")\n",
        "        print(f\"   Duration: ~{duration:.1f}s ({config['num_frames']} frames)\")\n",
        "        print(f\"   Steps: {config['num_inference_steps']}\")\n",
        "        print(f\"   üí° {config['description']}\")\n",
        "\n",
        "print_config_options()\n",
        "\n",
        "# Memory usage estimates\n",
        "print(\"\\nüíæ Estimated Memory Usage:\")\n",
        "print(\"   High Quality: ~35-38GB (may require cleanup between generations)\")\n",
        "print(\"   Balanced: ~25-30GB (recommended for most use cases)\")\n",
        "print(\"   Fast: ~15-20GB (reliable for multiple generations)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generation-header"
      },
      "source": [
        "## üé¨ 6. Interactive Video Generation\n",
        "\n",
        "Generate high-quality videos with HunyuanVideo using your custom prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generation-function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c16609-644a-443d-e5c8-d177385b8092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé¨ Video generation function ready!\n",
            "üìù Use generate_video_colab(prompt, quality) to create videos\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Video, display\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "def generate_video_colab(\n",
        "    prompt: str,\n",
        "    quality: str = \"balanced\",\n",
        "    seed: int = None,\n",
        "    output_dir: str = \"/content/videos\"\n",
        ") -> str:\n",
        "    \"\"\"Generate video with HunyuanVideo optimized for Colab A100.\n",
        "\n",
        "    Args:\n",
        "        prompt: Text description of the video to generate\n",
        "        quality: 'high_quality', 'balanced', or 'fast'\n",
        "        seed: Random seed for reproducible results\n",
        "        output_dir: Directory to save generated videos\n",
        "\n",
        "    Returns:\n",
        "        Path to generated video file\n",
        "    \"\"\"\n",
        "    pipe.vae.enable_tiling()\n",
        "    pipe.enable_model_cpu_offload()\n",
        "    if quality not in COLAB_CONFIGS:\n",
        "        quality = \"balanced\"\n",
        "        print(f\"‚ö†Ô∏è Invalid quality setting, using 'balanced'\")\n",
        "\n",
        "    config = COLAB_CONFIGS[quality]\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Generate filename with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"hunyuan_{quality}_{timestamp}.mp4\"\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    print(f\"üé¨ Generating video with {quality} settings...\")\n",
        "    print(f\"üìù Prompt: {prompt}\")\n",
        "    print(f\"üéØ Resolution: {config['width']}x{config['height']}\")\n",
        "    print(f\"‚è±Ô∏è Frames: {config['num_frames']} (~{config['num_frames']/8:.1f}s)\")\n",
        "\n",
        "    print_memory_status(\"(Before generation)\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Set random seed for reproducibility\n",
        "        generator = None\n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "            print(f\"üé≤ Using seed: {seed}\")\n",
        "\n",
        "        # Generate video with progress tracking\n",
        "        print(\"\\nüîÑ Generating frames...\")\n",
        "        video = pipe(\n",
        "            prompt=prompt,\n",
        "            height=config[\"height\"],\n",
        "            width=config[\"width\"],\n",
        "            num_frames=config[\"num_frames\"],\n",
        "            num_inference_steps=config[\"num_inference_steps\"],\n",
        "        ).frames[0]\n",
        "\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Save video using imageio with H.264 codec\n",
        "        print(\"\\nüíæ Saving video...\")\n",
        "        import imageio\n",
        "        with imageio.get_writer(output_path, fps=8, codec='h264', quality=8) as writer:\n",
        "            for frame in video:\n",
        "                writer.append_data(frame)\n",
        "\n",
        "        # Get file size\n",
        "        file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB\n",
        "\n",
        "        print(f\"\\n‚úÖ Video generated successfully!\")\n",
        "        print(f\"   ‚è±Ô∏è Generation time: {generation_time:.1f}s\")\n",
        "        print(f\"   üíæ File size: {file_size:.1f} MB\")\n",
        "        print(f\"   üìÅ Saved to: {output_path}\")\n",
        "\n",
        "        print_memory_status(\"(After generation)\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        print(\"\\n‚ùå GPU out of memory!\")\n",
        "        cleanup_memory()\n",
        "        print(\"\\nüí° Try these solutions:\")\n",
        "        print(\"   1. Use 'fast' quality setting\")\n",
        "        print(\"   2. Reduce num_frames or resolution\")\n",
        "        print(\"   3. Restart runtime to clear memory\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Generation failed: {str(e)}\")\n",
        "        cleanup_memory()\n",
        "        raise\n",
        "\n",
        "print(\"üé¨ Video generation function ready!\")\n",
        "print(\"üìù Use generate_video_colab(prompt, quality) to create videos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "examples-header"
      },
      "source": [
        "## üé® 7. Example Generations\n",
        "\n",
        "Try these example prompts or create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "4e2dc4b149134505b44e4f9a1ba29005",
            "96880d24152a470fb4be5fa749d0dd22",
            "b992084010a442a8b1e73679ae8e430c",
            "15a40a2a86b74936939076029bfdb3ee",
            "954347d6d0964a5cb539547cc1f00713",
            "c4b7f2f4448a493fa5ba489818f6373b",
            "2140c7b20fb1449f90a8a581ad966117",
            "625abc328f2b4930ab43501d7bc48221",
            "98db01c514ec479d8497d60a1185d840",
            "01b387c0713146b7b063c4fd422ae53e",
            "389dac3ff7d5411ebb2ad8cfa436d433"
          ]
        },
        "outputId": "05d66eca-4ea1-4d74-d307-7704cc14dc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé¨ Generating video with balanced settings...\n",
            "üìù Prompt: A majestic golden eagle soaring over snow-capped mountains at sunset, cinematic camera movement, high detail, beautiful lighting\n",
            "üéØ Resolution: 960x544\n",
            "‚è±Ô∏è Frames: 65 (~8.1s)\n",
            "üß† GPU Memory (Before generation):\n",
            "   Total: 39.6 GB\n",
            "   Allocated: 0.0 GB\n",
            "   Cached: 0.0 GB\n",
            "   Free: 39.6 GB\n",
            "   üöÄ Excellent memory - can use higher quality settings\n",
            "üé≤ Using seed: 42\n",
            "\n",
            "üîÑ Generating frames...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e2dc4b149134505b44e4f9a1ba29005"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Example 1: Balanced quality generation\n",
        "prompt_1 = \"A majestic golden eagle soaring over snow-capped mountains at sunset, cinematic camera movement, high detail, beautiful lighting\"\n",
        "\n",
        "video_path_1 = generate_video_colab(\n",
        "    prompt=prompt_1,\n",
        "    quality=\"balanced\",\n",
        "    seed=42  # For reproducible results\n",
        ")\n",
        "\n",
        "# Display the generated video\n",
        "print(\"\\nüé• Generated Video:\")\n",
        "display(Video(video_path_1, width=600))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example-2"
      },
      "outputs": [],
      "source": [
        "# Example 2: High quality generation (use with caution on memory)\n",
        "prompt_2 = \"A futuristic cyberpunk city at night with neon lights reflecting in rain puddles, flying cars in the distance, dramatic atmosphere\"\n",
        "\n",
        "# Check memory before high-quality generation\n",
        "mem = get_gpu_memory()\n",
        "if mem['free'] > 25:\n",
        "    print(\"üöÄ Sufficient memory detected - proceeding with high quality\")\n",
        "    video_path_2 = generate_video_colab(\n",
        "        prompt=prompt_2,\n",
        "        quality=\"high_quality\",\n",
        "        seed=123\n",
        "    )\n",
        "    display(Video(video_path_2, width=600))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Limited memory - using balanced quality instead\")\n",
        "    video_path_2 = generate_video_colab(\n",
        "        prompt=prompt_2,\n",
        "        quality=\"balanced\",\n",
        "        seed=123\n",
        "    )\n",
        "    display(Video(video_path_2, width=600))\n",
        "\n",
        "# Cleanup memory after generation\n",
        "cleanup_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom-generation"
      },
      "outputs": [],
      "source": [
        "# Custom generation - Enter your own prompt!\n",
        "custom_prompt = input(\"Enter your video prompt: \")\n",
        "quality_choice = input(\"Choose quality (high_quality/balanced/fast): \") or \"balanced\"\n",
        "custom_seed = input(\"Enter seed (or press enter for random): \")\n",
        "\n",
        "seed_value = None\n",
        "if custom_seed.strip():\n",
        "    try:\n",
        "        seed_value = int(custom_seed)\n",
        "    except ValueError:\n",
        "        print(\"Invalid seed, using random\")\n",
        "\n",
        "if custom_prompt.strip():\n",
        "    custom_video_path = generate_video_colab(\n",
        "        prompt=custom_prompt,\n",
        "        quality=quality_choice,\n",
        "        seed=seed_value\n",
        "    )\n",
        "    display(Video(custom_video_path, width=600))\n",
        "else:\n",
        "    print(\"No prompt entered, skipping generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch-header"
      },
      "source": [
        "## üìö 8. Batch Generation\n",
        "\n",
        "Generate multiple videos with automatic memory management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch-generation"
      },
      "outputs": [],
      "source": [
        "def batch_generate_videos(prompts_and_configs: list, output_dir: str = \"/content/batch_videos\"):\n",
        "    \"\"\"Generate multiple videos with automatic memory cleanup.\n",
        "\n",
        "    Args:\n",
        "        prompts_and_configs: List of (prompt, quality, seed) tuples\n",
        "        output_dir: Directory for batch outputs\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    results = []\n",
        "\n",
        "    print(f\"üé¨ Starting batch generation of {len(prompts_and_configs)} videos...\")\n",
        "\n",
        "    for i, config in enumerate(prompts_and_configs, 1):\n",
        "        prompt = config[0]\n",
        "        quality = config[1] if len(config) > 1 else \"balanced\"\n",
        "        seed = config[2] if len(config) > 2 else None\n",
        "\n",
        "        print(f\"\\nüìπ Generation {i}/{len(prompts_and_configs)}:\")\n",
        "        print(f\"   Prompt: {prompt[:60]}...\" if len(prompt) > 60 else f\"   Prompt: {prompt}\")\n",
        "\n",
        "        try:\n",
        "            video_path = generate_video_colab(\n",
        "                prompt=prompt,\n",
        "                quality=quality,\n",
        "                seed=seed,\n",
        "                output_dir=output_dir\n",
        "            )\n",
        "            results.append((prompt, video_path, \"success\"))\n",
        "\n",
        "            # Cleanup memory between generations\n",
        "            if i < len(prompts_and_configs):  # Don't cleanup after last generation\n",
        "                print(\"üßπ Cleaning up memory for next generation...\")\n",
        "                cleanup_memory()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed: {str(e)}\")\n",
        "            results.append((prompt, None, f\"error: {str(e)}\"))\n",
        "            cleanup_memory()  # Always cleanup on error\n",
        "\n",
        "    print(f\"\\n‚úÖ Batch generation completed!\")\n",
        "    print(f\"üìä Results: {sum(1 for r in results if r[2] == 'success')}/{len(results)} successful\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example batch generation\n",
        "sample_batch = [\n",
        "    (\"A cat playing with a ball of yarn in slow motion\", \"fast\", 111),\n",
        "    (\"Ocean waves crashing on a rocky shore at dawn\", \"balanced\", 222),\n",
        "    (\"A hummingbird feeding from colorful flowers\", \"balanced\", 333)\n",
        "]\n",
        "\n",
        "print(\"üé¨ Example batch ready to run!\")\n",
        "print(\"üìù Uncomment the line below to start batch generation:\")\n",
        "print(\"# batch_results = batch_generate_videos(sample_batch)\")\n",
        "\n",
        "# Uncomment to run:\n",
        "# batch_results = batch_generate_videos(sample_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export-header"
      },
      "source": [
        "## üíæ 9. Export & Download\n",
        "\n",
        "Export your generated videos to Google Drive or download them directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for permanent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def copy_to_drive(video_path: str, drive_folder: str = \"HunyuanVideo_Outputs\"):\n",
        "    \"\"\"Copy generated video to Google Drive.\"\"\"\n",
        "    import shutil\n",
        "\n",
        "    drive_path = f\"/content/drive/MyDrive/{drive_folder}\"\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    filename = os.path.basename(video_path)\n",
        "    drive_file = os.path.join(drive_path, filename)\n",
        "\n",
        "    shutil.copy2(video_path, drive_file)\n",
        "    print(f\"‚úÖ Video copied to Google Drive: {drive_file}\")\n",
        "    return drive_file\n",
        "\n",
        "def download_video(video_path: str):\n",
        "    \"\"\"Download video to local machine.\"\"\"\n",
        "    from google.colab import files\n",
        "    if os.path.exists(video_path):\n",
        "        files.download(video_path)\n",
        "        print(f\"‚¨áÔ∏è Download started for: {os.path.basename(video_path)}\")\n",
        "    else:\n",
        "        print(f\"‚ùå File not found: {video_path}\")\n",
        "\n",
        "print(\"üíæ Google Drive mounted successfully!\")\n",
        "print(\"üìÅ Use copy_to_drive(video_path) to save to Drive\")\n",
        "print(\"‚¨áÔ∏è Use download_video(video_path) to download directly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list-generated"
      },
      "outputs": [],
      "source": [
        "# List all generated videos\n",
        "import glob\n",
        "\n",
        "video_dirs = [\"/content/videos\", \"/content/batch_videos\"]\n",
        "all_videos = []\n",
        "\n",
        "for video_dir in video_dirs:\n",
        "    if os.path.exists(video_dir):\n",
        "        videos = glob.glob(os.path.join(video_dir, \"*.mp4\"))\n",
        "        all_videos.extend(videos)\n",
        "\n",
        "if all_videos:\n",
        "    print(f\"üé¨ Found {len(all_videos)} generated videos:\")\n",
        "    for i, video_path in enumerate(all_videos, 1):\n",
        "        file_size = os.path.getsize(video_path) / (1024 * 1024)\n",
        "        print(f\"   {i}. {os.path.basename(video_path)} ({file_size:.1f} MB)\")\n",
        "\n",
        "    print(\"\\nüíæ Export options:\")\n",
        "    print(\"   # Copy all videos to Google Drive\")\n",
        "    print(\"   for video in all_videos: copy_to_drive(video)\")\n",
        "    print(\"\")\n",
        "    print(\"   # Download specific video (replace index)\")\n",
        "    print(\"   download_video(all_videos[0])  # Downloads first video\")\n",
        "else:\n",
        "    print(\"üì≠ No videos found. Generate some videos first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export-all"
      },
      "outputs": [],
      "source": [
        "# Quick export all videos to Drive\n",
        "if all_videos:\n",
        "    export_choice = input(\"Export all videos to Google Drive? (y/n): \")\n",
        "    if export_choice.lower() == 'y':\n",
        "        print(\"üì§ Exporting all videos to Google Drive...\")\n",
        "        for video_path in all_videos:\n",
        "            try:\n",
        "                copy_to_drive(video_path)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Failed to copy {video_path}: {e}\")\n",
        "        print(\"‚úÖ Export completed!\")\n",
        "    else:\n",
        "        print(\"üìã Export skipped. Use copy_to_drive(video_path) for individual files.\")\n",
        "else:\n",
        "    print(\"üì≠ No videos to export.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting-header"
      },
      "source": [
        "## üîß 10. Troubleshooting & Optimization\n",
        "\n",
        "Common issues and solutions for HunyuanVideo on Colab A100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diagnostics"
      },
      "outputs": [],
      "source": [
        "def run_diagnostics():\n",
        "    \"\"\"Run comprehensive system diagnostics.\"\"\"\n",
        "    print(\"üîç Running HunyuanVideo Colab Diagnostics...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # GPU Information\n",
        "    print(\"\\nüñ•Ô∏è GPU Information:\")\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_props = torch.cuda.get_device_properties(0)\n",
        "        print(f\"   GPU: {gpu_props.name}\")\n",
        "        print(f\"   Total Memory: {gpu_props.total_memory / 1024**3:.1f} GB\")\n",
        "        print(f\"   CUDA Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
        "\n",
        "        # Check if A100\n",
        "        if \"A100\" in gpu_props.name:\n",
        "            print(\"   ‚úÖ A100 GPU detected - optimal for HunyuanVideo\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è Non-A100 GPU - may need optimization\")\n",
        "    else:\n",
        "        print(\"   ‚ùå No GPU available\")\n",
        "\n",
        "    # Memory Status\n",
        "    print_memory_status(\"\\nüß† Current Memory\")\n",
        "\n",
        "    # Disk Space\n",
        "    print(\"\\nüíæ Disk Space:\")\n",
        "    import shutil\n",
        "    total, used, free = shutil.disk_usage(\"/content\")\n",
        "    print(f\"   Total: {total / 1024**3:.1f} GB\")\n",
        "    print(f\"   Used: {used / 1024**3:.1f} GB\")\n",
        "    print(f\"   Free: {free / 1024**3:.1f} GB\")\n",
        "\n",
        "    if free / 1024**3 < 20:\n",
        "        print(\"   ‚ö†Ô∏è Low disk space - may affect model loading\")\n",
        "\n",
        "    # Library Versions\n",
        "    print(\"\\nüìö Library Versions:\")\n",
        "    try:\n",
        "        import diffusers, transformers, torch\n",
        "        print(f\"   PyTorch: {torch.__version__}\")\n",
        "        print(f\"   Diffusers: {diffusers.__version__}\")\n",
        "        print(f\"   Transformers: {transformers.__version__}\")\n",
        "\n",
        "        # Check versions\n",
        "        if diffusers.__version__ >= \"0.33.1\":\n",
        "            print(\"   ‚úÖ Diffusers version supports HunyuanVideo\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è Update diffusers: pip install diffusers>=0.33.1\")\n",
        "    except ImportError as e:\n",
        "        print(f\"   ‚ùå Import error: {e}\")\n",
        "\n",
        "    # Model Status\n",
        "    print(\"\\nü§ñ Model Status:\")\n",
        "    try:\n",
        "        # Check if pipe is loaded\n",
        "        if 'pipe' in globals():\n",
        "            print(\"   ‚úÖ HunyuanVideo pipeline loaded\")\n",
        "            print(f\"   Device: {next(pipe.parameters()).device}\")\n",
        "            print(f\"   Dtype: {next(pipe.parameters()).dtype}\")\n",
        "        else:\n",
        "            print(\"   ‚ùå HunyuanVideo pipeline not loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Error checking model: {e}\")\n",
        "\n",
        "    print(\"\\nüîß Optimization Status:\")\n",
        "    print(f\"   TF32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")\n",
        "    print(f\"   cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
        "    print(f\"   Memory fraction: {torch.cuda.get_memory_fraction(0) if torch.cuda.is_available() else 'N/A'}\")\n",
        "\n",
        "def memory_optimization_tips():\n",
        "    \"\"\"Show memory optimization tips.\"\"\"\n",
        "    print(\"üí° Memory Optimization Tips:\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"\\nüîß If you get Out of Memory errors:\")\n",
        "    print(\"   1. Use 'fast' quality setting\")\n",
        "    print(\"   2. Reduce num_frames (try 32 instead of 65)\")\n",
        "    print(\"   3. Lower resolution (512x512 instead of 720p)\")\n",
        "    print(\"   4. Restart runtime to clear memory\")\n",
        "    print(\"   5. Run cleanup_memory() between generations\")\n",
        "\n",
        "    print(\"\\n‚ö° For better performance:\")\n",
        "    print(\"   1. Use sequential_cpu_offload (already enabled)\")\n",
        "    print(\"   2. Enable VAE tiling (already enabled)\")\n",
        "    print(\"   3. Use FP16 precision (already enabled)\")\n",
        "    print(\"   4. Generate videos one at a time\")\n",
        "    print(\"   5. Close browser tabs to free system RAM\")\n",
        "\n",
        "# Run diagnostics\n",
        "run_diagnostics()\n",
        "memory_optimization_tips()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Clear Python variables\n",
        "if 'pipe' in globals():\n",
        "    print(\"   Clearing pipeline...\")\n",
        "    del pipe\n",
        "\n",
        "# Garbage collection\n",
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "IpgWxnmwwLSa",
        "outputId": "0f806329-98f6-4925-85a4-b82fee4ee891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # CUDA cleanup\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"   CUDA cache cleared\")"
      ],
      "metadata": {
        "id": "4ERAuhHhyIoJ",
        "outputId": "be33c13f-4eba-4e59-e9be-c55e079c3686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   CUDA cache cleared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emergency-cleanup"
      },
      "outputs": [],
      "source": [
        "# Emergency memory cleanup\n",
        "def emergency_cleanup():\n",
        "    \"\"\"Aggressive memory cleanup for emergency situations.\"\"\"\n",
        "    print(\"üö® Running emergency memory cleanup...\")\n",
        "\n",
        "    # Clear Python variables\n",
        "    if 'pipe' in globals():\n",
        "        print(\"   Clearing pipeline...\")\n",
        "        del pipe\n",
        "\n",
        "    # Garbage collection\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    # CUDA cleanup\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "        print(\"   CUDA cache cleared\")\n",
        "\n",
        "    # Check memory after cleanup\n",
        "    print_memory_status(\"(After cleanup)\")\n",
        "\n",
        "    print(\"‚úÖ Emergency cleanup completed\")\n",
        "    print(\"üí° You may need to reload the model with the loading cell above\")\n",
        "\n",
        "print(\"üö® Emergency cleanup function ready\")\n",
        "print(\"üìù Run emergency_cleanup() if you encounter memory issues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion-header"
      },
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "Congratulations! You've successfully set up and used HunyuanVideo on Google Colab A100.\n",
        "\n",
        "### üéØ What You've Accomplished:\n",
        "- ‚úÖ Loaded a 13B parameter video generation model on Colab\n",
        "- ‚úÖ Optimized for A100 40GB memory constraints\n",
        "- ‚úÖ Generated high-quality videos up to 15 seconds\n",
        "- ‚úÖ Learned memory management for large AI models\n",
        "- ‚úÖ Set up batch generation and export workflows\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "1. **Experiment** with different prompts and quality settings\n",
        "2. **Explore** the full videogenbook library for more models\n",
        "3. **Read** the complete guide: *Hands-On Video Generation with AI*\n",
        "4. **Join** the community: [GitHub Repository](https://github.com/jenochs/video-generation-book)\n",
        "\n",
        "### üîó Resources:\n",
        "- **Documentation**: [videogenbook docs](https://github.com/jenochs/video-generation-book)\n",
        "- **HunyuanVideo**: [Model Card](https://huggingface.co/tencent/HunyuanVideo)\n",
        "- **Colab Pro+**: [Upgrade for A100 access](https://colab.research.google.com/signup)\n",
        "\n",
        "### üí° Tips for Best Results:\n",
        "- Use detailed, descriptive prompts\n",
        "- Include camera movement descriptions\n",
        "- Specify lighting and atmosphere\n",
        "- Experiment with different seeds for variety\n",
        "- Monitor memory usage for stable generation\n",
        "\n",
        "**Happy video generating! üé¨‚ú®**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e647f78828394bada20a973765802617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92070acddab4441186ea839bcac86e5f",
              "IPY_MODEL_6d3d3c02d4fb4cc88a9dcbf59ab41b2d",
              "IPY_MODEL_63150f18d5b94a9691e39bf4325b8d2f"
            ],
            "layout": "IPY_MODEL_b8d38c80fe4f4bd98ea89af12988e29a"
          }
        },
        "92070acddab4441186ea839bcac86e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_621f031f5691495b9bbfbdcadcbf3626",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9fb99b56e2c441d1b5f0c96f552a4188",
            "value": "Loading‚Äápipeline‚Äácomponents...:‚Äá100%"
          }
        },
        "6d3d3c02d4fb4cc88a9dcbf59ab41b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40997681347407c9bfa0e47cd2de4e3",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db9e5ce6b76448c68b9672d09853ef21",
            "value": 7
          }
        },
        "63150f18d5b94a9691e39bf4325b8d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38035ea5994b4e708a4d14a679bcc0eb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a77c3a0365c04cb78c0e6d715df444fc",
            "value": "‚Äá7/7‚Äá[00:47&lt;00:00,‚Äá‚Äá3.57s/it]"
          }
        },
        "b8d38c80fe4f4bd98ea89af12988e29a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "621f031f5691495b9bbfbdcadcbf3626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fb99b56e2c441d1b5f0c96f552a4188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a40997681347407c9bfa0e47cd2de4e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9e5ce6b76448c68b9672d09853ef21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38035ea5994b4e708a4d14a679bcc0eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77c3a0365c04cb78c0e6d715df444fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbae256f1a89414fbd5fd34277cd1dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1931ef55e2804a3786672074445dea96",
              "IPY_MODEL_3d9a9e30412e43df85b8934ebe1d3b25",
              "IPY_MODEL_acf92a9603c645f09c28a637beb48eb9"
            ],
            "layout": "IPY_MODEL_c8cb034ee86c4ad39ee723ec70a083dc"
          }
        },
        "1931ef55e2804a3786672074445dea96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab52539a99d04d09adb9923898ec61b6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_981a220ac09e47a39a987e3b58042c99",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "3d9a9e30412e43df85b8934ebe1d3b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb1aeb42fea04e39be432975a1cf62c6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcd5bb0d262a4c059ebadad6ab99ae84",
            "value": 4
          }
        },
        "acf92a9603c645f09c28a637beb48eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48e1606f81d54af99132275bc8027672",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ea165d4c03364d2084063ad6cc747f52",
            "value": "‚Äá4/4‚Äá[00:38&lt;00:00,‚Äá‚Äá8.77s/it]"
          }
        },
        "c8cb034ee86c4ad39ee723ec70a083dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab52539a99d04d09adb9923898ec61b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "981a220ac09e47a39a987e3b58042c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb1aeb42fea04e39be432975a1cf62c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcd5bb0d262a4c059ebadad6ab99ae84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48e1606f81d54af99132275bc8027672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea165d4c03364d2084063ad6cc747f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45b986ddc2a445f3bab05b18fe6f147a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d5919838f2d4aebbcf1987a8eff045f",
              "IPY_MODEL_1e7f9c32d1f64e6fabd46e87b5668b5c",
              "IPY_MODEL_fd29bc0f9f91436c866d8f33b3037fe4"
            ],
            "layout": "IPY_MODEL_51f79834e4634beaab90d6773d82febb"
          }
        },
        "7d5919838f2d4aebbcf1987a8eff045f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a53410330e640bc9c4cb1b3b225745c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_05310a718fab4619940e4ff8c99ca2c2",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "1e7f9c32d1f64e6fabd46e87b5668b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9af9dc1481b640d2983ae5dfc43150a6",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b4caf3065cd44f796ac4c6cc3de509c",
            "value": 6
          }
        },
        "fd29bc0f9f91436c866d8f33b3037fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ba5f136e4284aefb777154a019831bd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d7840f70f78749a0b719b0dc60d23743",
            "value": "‚Äá6/6‚Äá[00:01&lt;00:00,‚Äá‚Äá3.50it/s]"
          }
        },
        "51f79834e4634beaab90d6773d82febb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a53410330e640bc9c4cb1b3b225745c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05310a718fab4619940e4ff8c99ca2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9af9dc1481b640d2983ae5dfc43150a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b4caf3065cd44f796ac4c6cc3de509c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ba5f136e4284aefb777154a019831bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7840f70f78749a0b719b0dc60d23743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e2dc4b149134505b44e4f9a1ba29005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96880d24152a470fb4be5fa749d0dd22",
              "IPY_MODEL_b992084010a442a8b1e73679ae8e430c",
              "IPY_MODEL_15a40a2a86b74936939076029bfdb3ee"
            ],
            "layout": "IPY_MODEL_954347d6d0964a5cb539547cc1f00713"
          }
        },
        "96880d24152a470fb4be5fa749d0dd22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b7f2f4448a493fa5ba489818f6373b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2140c7b20fb1449f90a8a581ad966117",
            "value": "‚Äá56%"
          }
        },
        "b992084010a442a8b1e73679ae8e430c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_625abc328f2b4930ab43501d7bc48221",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98db01c514ec479d8497d60a1185d840",
            "value": 14
          }
        },
        "15a40a2a86b74936939076029bfdb3ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01b387c0713146b7b063c4fd422ae53e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_389dac3ff7d5411ebb2ad8cfa436d433",
            "value": "‚Äá14/25‚Äá[04:16&lt;02:40,‚Äá14.60s/it]"
          }
        },
        "954347d6d0964a5cb539547cc1f00713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4b7f2f4448a493fa5ba489818f6373b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2140c7b20fb1449f90a8a581ad966117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "625abc328f2b4930ab43501d7bc48221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98db01c514ec479d8497d60a1185d840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01b387c0713146b7b063c4fd422ae53e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "389dac3ff7d5411ebb2ad8cfa436d433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}