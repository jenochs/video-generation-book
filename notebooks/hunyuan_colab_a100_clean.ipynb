{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": "# HunyuanVideo on Google Colab A100\\n\\n**Generate high-quality videos using Tencent's official HunyuanVideo model on Google Colab A100 GPU**\\n\\n\ud83d\ude80 **What you'll learn:**\\n- Run the official 13B parameter HunyuanVideo model on Colab A100 (40GB)\\n- Use exact package versions for maximum compatibility\\n- Optimize memory usage for large-scale video generation\\n- Generate videos up to 15 seconds with advanced prompting\\n- Export and download high-quality video results\\n\\n\u26a1 **Requirements:**\\n- Google Colab Pro+ with A100 GPU access\\n- ~20-30 minutes for complete setup\\n- Google Drive for video storage (optional)\\n\\n\ud83d\udcda **Model Info:** Official tencent/HunyuanVideo (integrated into Diffusers Dec 17, 2024)\\n\ud83d\udcd6 **From the Book:** *Hands-On Video Generation with AI* - Chapter 3: Advanced Model Implementation\""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## \ud83d\udd27 1. Environment Setup & GPU Verification\n",
        "\n",
        "First, let's verify we have an A100 GPU and configure the environment for optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpu-check",
        "outputId": "5039383e-aeee-4a87-88df-7a1d076a9b8c"
      },
      "outputs": [],
      "source": "# Check GPU availability and specifications\\n!nvidia-smi\\n\\n# Verify we have A100 access\\nimport subprocess\\nresult = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'],\\n                       capture_output=True, text=True)\\nprint(\\\"GPU Information:\\\")\\ngpu_info = result.stdout.strip().split(', ')\\nif len(gpu_info) >= 2:\\n    gpu_name, gpu_memory = gpu_info[0], int(gpu_info[1])\\n    print(f\\\"   GPU: {gpu_name}\\\")\\n    print(f\\\"   Memory: {gpu_memory:,} MB ({gpu_memory/1024:.1f} GB)\\\")\\n\\n    if \\\"A100\\\" in gpu_name and gpu_memory >= 40000:\\n        print(\\\"   \u2705 Perfect! A100 40GB detected - optimal for HunyuanVideo\\\")\\n    elif \\\"A100\\\" in gpu_name:\\n        print(\\\"   \u26a0\ufe0f A100 detected but check memory - may need optimization\\\")\\n    else:\\n        print(\\\"   \u274c Warning: A100 GPU recommended for best performance\\\")\\n        print(\\\"   \ud83d\udca1 Consider upgrading to Colab Pro+ for A100 access\\\")\\nelse:\\n    print(\\\"   \u274c Unable to detect GPU information\\\")\"",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-header"
      },
      "source": "## \ud83d\udce6 2. Install Dependencies\\n\\nInstall the exact package versions required by HunyuanVideo (official requirements from Dec 2024).\\n\\n**Important**: These specific versions are tested and verified to work with HunyuanVideo.\""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install-pytorch",
        "outputId": "e01ed6b9-f9fe-4778-e5a0-8c8c14fed5f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch with CUDA 12.1 support (optimized for A100)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install-diffusers",
        "outputId": "dc794863-3565-425c-eb18-1ce0532036bc"
      },
      "outputs": [],
      "source": [
        "# Install HunyuanVideo dependencies (exact official versions)\\n!pip install diffusers==0.31.0  # Official HunyuanVideo requirement\\n!pip install transformers==4.46.3  # Official HunyuanVideo requirement  \\n!pip install accelerate==1.1.1  # Official HunyuanVideo requirement\\n!pip install safetensors==0.4.5 tokenizers==0.20.3  # Official supporting versions\\n!pip install xformers  # Latest version for A100 optimization\\n!pip install imageio-ffmpeg einops  # For video processing and tensor operations\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install-videogenbook",
        "outputId": "dfd75522-216f-4fb3-c30f-d42fcf88feef"
      },
      "outputs": [],
      "source": [
        "# Verify installation and imports\\nimport torch\\nimport diffusers\\nimport transformers\\nimport accelerate\\n\\nprint(\\\"\u2705 Package Installation Verified:\\\")\\nprint(f\\\"   PyTorch: {torch.__version__}\\\")\\nprint(f\\\"   Diffusers: {diffusers.__version__}\\\")\\nprint(f\\\"   Transformers: {transformers.__version__}\\\")\\nprint(f\\\"   Accelerate: {accelerate.__version__}\\\")\\nprint(f\\\"   CUDA available: {torch.cuda.is_available()}\\\")\\n\\n# Check if versions match HunyuanVideo requirements\\nif diffusers.__version__ >= \\\"0.31.0\\\":\\n    print(\\\"   \u2705 Diffusers version supports HunyuanVideo\\\")\\nelse:\\n    print(\\\"   \u26a0\ufe0f Diffusers version may not support HunyuanVideo\\\")\\n\\nif transformers.__version__ >= \\\"4.46.3\\\":\\n    print(\\\"   \u2705 Transformers version compatible\\\")\\nelse:\\n    print(\\\"   \u26a0\ufe0f Transformers version may need update\\\")\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env-config"
      },
      "outputs": [],
      "source": [
        "# Configure environment for maximum memory efficiency\\nimport os\\nimport torch\\n\\n# Essential memory optimizations for A100 40GB\\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\\nos.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # Async for better performance\\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid warnings\\n\\n# Enable optimized math operations\\ntorch.backends.cuda.matmul.allow_tf32 = True\\ntorch.backends.cudnn.allow_tf32 = True\\ntorch.backends.cudnn.benchmark = True\\n\\nprint(\\\"\ud83d\udd27 Environment configured for A100 optimization\\\")\\nprint(f\\\"   PyTorch version: {torch.__version__}\\\")\\nprint(f\\\"   CUDA available: {torch.cuda.is_available()}\\\")\\nif torch.cuda.is_available():\\n    print(f\\\"   CUDA version: {torch.version.cuda}\\\")\\n    print(f\\\"   GPU count: {torch.cuda.device_count()}\\\")\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memory-header"
      },
      "source": [
        "## \ud83e\udde0 3. Memory Monitoring & Optimization\n",
        "\n",
        "Set up memory monitoring and configure HunyuanVideo for A100 40GB constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-utils"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from typing import Dict, Any\n",
        "\n",
        "def get_gpu_memory() -> Dict[str, float]:\n",
        "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return {\"total\": 0, \"used\": 0, \"free\": 0}\n",
        "\n",
        "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    cached = torch.cuda.memory_reserved() / 1024**3\n",
        "    free = total - cached\n",
        "\n",
        "    return {\n",
        "        \"total\": total,\n",
        "        \"allocated\": allocated,\n",
        "        \"cached\": cached,\n",
        "        \"free\": free\n",
        "    }\n",
        "\n",
        "def print_memory_status(stage: str = \"\"):\n",
        "    \"\"\"Print current memory status.\"\"\"\n",
        "    mem = get_gpu_memory()\n",
        "    print(f\"\ud83e\udde0 GPU Memory {stage}:\")\n",
        "    print(f\"   Total: {mem['total']:.1f} GB\")\n",
        "    print(f\"   Allocated: {mem['allocated']:.1f} GB\")\n",
        "    print(f\"   Cached: {mem['cached']:.1f} GB\")\n",
        "    print(f\"   Free: {mem['free']:.1f} GB\")\n",
        "\n",
        "    # Memory warnings\n",
        "    if mem['free'] < 10:\n",
        "        print(\"   \u26a0\ufe0f Low memory - consider reducing resolution/frames\")\n",
        "    elif mem['free'] < 20:\n",
        "        print(\"   \u2705 Sufficient memory for standard generation\")\n",
        "    else:\n",
        "        print(\"   \ud83d\ude80 Excellent memory - can use higher quality settings\")\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Cleanup GPU memory.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "# Initial memory check\n",
        "print_memory_status(\"(Initial)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-header"
      },
      "source": "## \ud83e\udd16 4. Load Official HunyuanVideo Model\\n\\nLoad the official Tencent HunyuanVideo model with A100-optimized settings for 40GB memory constraint.\\n\\n**Note**: Using official `tencent/HunyuanVideo` model path (integrated into Diffusers Dec 17, 2024).\""
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load-model"
      },
      "outputs": [],
      "source": "# Load HunyuanVideo model with A100 optimizations\\nfrom diffusers import HunyuanVideoPipeline\\nimport torch\\nimport time\\n\\nprint(\\\"\ud83d\udd04 Loading HunyuanVideo model (this may take 5-10 minutes)...\\\")\\nprint(\\\"\ud83d\udce5 Downloading ~26GB of model weights...\\\")\\n\\nstart_time = time.time()\\n\\ntry:\\n    # Load official HunyuanVideo model (integrated Dec 17, 2024)\\n    pipe = HunyuanVideoPipeline.from_pretrained(\\n        \\\"tencent/HunyuanVideo\\\",  # Official model path (not community fork)\\n        torch_dtype=torch.bfloat16,  # Official recommendation: Transformer in bfloat16\\n        use_safetensors=True,\\n        low_cpu_mem_usage=True,     # Minimize CPU memory during loading\\n    )\\n\\n    print(\\\"Applying A100 optimizations...\\\")\\n\\n    # Essential memory optimizations for 40GB constraint\\n    pipe.enable_sequential_cpu_offload()  # Most aggressive memory optimization\\n    pipe.vae.enable_tiling()              # Reduce VAE memory usage\\n    pipe.vae.enable_slicing()             # Further VAE optimization\\n\\n    # Set optimal dtypes as per official docs\\n    # VAE and text encoders should be in float16, Transformer in bfloat16\\n    pipe.vae = pipe.vae.to(torch.float16)\\n    if hasattr(pipe, 'text_encoder'):\\n        pipe.text_encoder = pipe.text_encoder.to(torch.float16)\\n    if hasattr(pipe, 'text_encoder_2'):\\n        pipe.text_encoder_2 = pipe.text_encoder_2.to(torch.float16)\\n\\n    # Enable memory-efficient attention if available\\n    try:\\n        pipe.enable_xformers_memory_efficient_attention()\\n        print(\\\"   \u2705 xFormers memory-efficient attention enabled\\\")\\n    except ImportError:\\n        print(\\\"   \u26a0\ufe0f xFormers not available - using default attention\\\")\\n    except Exception as e:\\n        print(f\\\"   \u26a0\ufe0f xFormers setup issue: {e}\\\")\\n\\n    load_time = time.time() - start_time\\n    print(f\\\"\u2705 HunyuanVideo loaded successfully in {load_time:.1f}s\\\")\\n    print(\\\"\ud83c\udfac Ready for video generation!\\\")\\n\\n    print_memory_status(\\\"(After model loading)\\\")\\n\\nexcept Exception as e:\\n    print(f\\\"\u274c Failed to load HunyuanVideo: {str(e)}\\\")\\n    print(\\\"Troubleshooting steps:\\\")\\n    print(\\\"1. Ensure you have A100 GPU access\\\")\\n    print(\\\"2. Check available disk space (need ~30GB)\\\")\\n    print(\\\"3. Restart runtime and try again\\\")\\n    print(\\\"4. Try using hunyuanvideo-community/HunyuanVideo if official fails\\\")\\n    raise\"",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "## \u2699\ufe0f 5. A100-Optimized Generation Settings\n",
        "\n",
        "Configure generation parameters optimized for A100 40GB memory constraints while maintaining high quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config-settings"
      },
      "outputs": [],
      "source": "# A100 40GB optimized configurations\\nCOLAB_CONFIGS = {\\n    \\\"high_quality\\\": {\\n        \\\"height\\\": 720,\\n        \\\"width\\\": 1280,\\n        \\\"num_frames\\\": 65,           # Reduced from 129 for memory\\n        \\\"guidance_scale\\\": 7.0,\\n        \\\"num_inference_steps\\\": 30,  # Balanced quality/speed\\n        \\\"description\\\": \\\"High quality 720p (may use significant memory)\\\"\\n    },\\n    \\\"balanced\\\": {\\n        \\\"height\\\": 544,\\n        \\\"width\\\": 960,\\n        \\\"num_frames\\\": 65,\\n        \\\"guidance_scale\\\": 7.0,\\n        \\\"num_inference_steps\\\": 25,\\n        \\\"description\\\": \\\"Balanced quality and memory usage (recommended)\\\"\\n    },\\n    \\\"fast\\\": {\\n        \\\"height\\\": 512,\\n        \\\"width\\\": 512,\\n        \\\"num_frames\\\": 32,\\n        \\\"guidance_scale\\\": 6.0,\\n        \\\"num_inference_steps\\\": 20,\\n        \\\"description\\\": \\\"Fast generation with lower memory usage\\\"\\n    }\\n}\\n\\ndef print_config_options():\\n    \\\"\\\"\\\"Display available configuration options.\\\"\\\"\\\"\\n    print(\\\"\ud83d\udcca Available Quality Configurations:\\\")\\n    for name, config in COLAB_CONFIGS.items():\\n        duration = config['num_frames'] / 8  # Assuming 8 FPS\\n        print(f\\\"\\\")\\n        print(f\\\"\ud83c\udfa5 {name.upper()}:\\\")\\n        print(f\\\"   Resolution: {config['width']}x{config['height']}\\\")\\n        print(f\\\"   Duration: ~{duration:.1f}s ({config['num_frames']} frames)\\\")\\n        print(f\\\"   Steps: {config['num_inference_steps']}\\\")\\n        print(f\\\"   \ud83d\udca1 {config['description']}\\\")\\n\\nprint_config_options()\\n\\n# Memory usage estimates\\nprint(\\\"\\\")\\nprint(\\\"\ud83d\udcbe Estimated Memory Usage:\\\")\\nprint(\\\"   High Quality: ~35-38GB (may require cleanup between generations)\\\")\\nprint(\\\"   Balanced: ~25-30GB (recommended for most use cases)\\\")\\nprint(\\\"   Fast: ~15-20GB (reliable for multiple generations)\\\")\"",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generation-header"
      },
      "source": [
        "## \ud83c\udfac 6. Interactive Video Generation\n",
        "\n",
        "Generate high-quality videos with HunyuanVideo using your custom prompts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generation-function"
      },
      "outputs": [],
      "source": "# Enhanced video generation function with correct output handling\\nfrom IPython.display import Video, display\\nimport os\\nfrom datetime import datetime\\nimport numpy as np\\n\\ndef generate_video_colab(\\n    prompt: str,\\n    quality: str = \\\"balanced\\\",\\n    seed: int = None,\\n    output_dir: str = \\\"/content/videos\\\"\\n) -> str:\\n    \\\"\\\"\\\"Generate video with HunyuanVideo optimized for Colab A100.\\n\\n    Args:\\n        prompt: Text description of the video to generate\\n        quality: 'high_quality', 'balanced', or 'fast'\\n        seed: Random seed for reproducible results\\n        output_dir: Directory to save generated videos\\n\\n    Returns:\\n        Path to generated video file\\n    \\\"\\\"\\\"\\n    if quality not in COLAB_CONFIGS:\\n        quality = \\\"balanced\\\"\\n        print(f\\\"\u26a0\ufe0f Invalid quality setting, using 'balanced'\\\")\\n\\n    config = COLAB_CONFIGS[quality]\\n\\n    # Create output directory\\n    os.makedirs(output_dir, exist_ok=True)\\n\\n    # Generate filename with timestamp\\n    timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n    filename = f\\\"hunyuan_{quality}_{timestamp}.mp4\\\"\\n    output_path = os.path.join(output_dir, filename)\\n\\n    print(f\\\"\ud83c\udfac Generating video with {quality} settings...\\\")\\n    print(f\\\"\ud83d\udcdd Prompt: {prompt}\\\")\\n    print(f\\\"\ud83c\udfaf Resolution: {config['width']}x{config['height']}\\\")\\n    print(f\\\"\u23f1\ufe0f Frames: {config['num_frames']} (~{config['num_frames']/8:.1f}s)\\\")\\n\\n    print_memory_status(\\\"(Before generation)\\\")\\n\\n    try:\\n        start_time = time.time()\\n\\n        # Set random seed for reproducibility\\n        generator = None\\n        if seed is not None:\\n            generator = torch.Generator(device=\\\"cuda\\\").manual_seed(seed)\\n            print(f\\\"\ud83c\udfb2 Using seed: {seed}\\\")\\n\\n        # Generate video with correct output format\\n        print(\\\"Generating frames...\\\")\\n        result = pipe(\\n            prompt=prompt,\\n            height=config[\\\"height\\\"],\\n            width=config[\\\"width\\\"],\\n            num_frames=config[\\\"num_frames\\\"],\\n            guidance_scale=config[\\\"guidance_scale\\\"],\\n            num_inference_steps=config[\\\"num_inference_steps\\\"],\\n            generator=generator,\\n            output_type=\\\"pil\\\",  # Explicit output type for PIL Images\\n            return_dict=True    # Return HunyuanVideoPipelineOutput object\\n        )\\n\\n        # Extract frames - correct format is result.frames[0]\\n        video_frames = result.frames[0]\\n        generation_time = time.time() - start_time\\n\\n        # Save video with proper PIL Image handling\\n        print(\\\"Saving video...\\\")\\n        import imageio\\n        \\n        # Convert PIL Images to numpy arrays for imageio\\n        frames_array = []\\n        for frame in video_frames:\\n            if hasattr(frame, 'convert'):  # PIL Image\\n                frame_array = np.array(frame.convert('RGB'))\\n            else:  # Already numpy array\\n                frame_array = frame\\n            frames_array.append(frame_array)\\n\\n        # Save with H.264 codec for compatibility\\n        with imageio.get_writer(output_path, fps=8, codec='h264', quality=8) as writer:\\n            for frame_array in frames_array:\\n                writer.append_data(frame_array)\\n\\n        # Get file size\\n        file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB\\n\\n        print(f\\\"\u2705 Video generated successfully!\\\")\\n        print(f\\\"   \u23f1\ufe0f Generation time: {generation_time:.1f}s\\\")\\n        print(f\\\"   \ud83c\udfac Frames generated: {len(video_frames)}\\\")\\n        print(f\\\"   \ud83d\udcbe File size: {file_size:.1f} MB\\\")\\n        print(f\\\"   \ud83d\udcc1 Saved to: {output_path}\\\")\\n\\n        print_memory_status(\\\"(After generation)\\\")\\n\\n        return output_path\\n\\n    except torch.cuda.OutOfMemoryError:\\n        print(\\\"\u274c GPU out of memory!\\\")\\n        cleanup_memory()\\n        print(\\\"\ud83d\udca1 Try these solutions:\\\")\\n        print(\\\"   1. Use 'fast' quality setting\\\")\\n        print(\\\"   2. Reduce num_frames or resolution\\\")\\n        print(\\\"   3. Restart runtime to clear memory\\\")\\n        raise\\n    except Exception as e:\\n        print(f\\\"\u274c Generation failed: {str(e)}\\\")\\n        cleanup_memory()\\n        raise\\n\\nprint(\\\"\ud83c\udfac Enhanced video generation function ready!\\\")\\nprint(\\\"\ud83d\udcdd Use generate_video_colab(prompt, quality) to create videos\\\")\"",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "examples-header"
      },
      "source": [
        "## \ud83c\udfa8 7. Example Generations\n",
        "\n",
        "Try these example prompts or create your own!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "example-1"
      },
      "outputs": [],
      "source": "# Example 1: Balanced quality generation\\nprompt_1 = \\\"A majestic golden eagle soaring over snow-capped mountains at sunset, cinematic camera movement, high detail, beautiful lighting\\\"\\n\\nvideo_path_1 = generate_video_colab(\\n    prompt=prompt_1,\\n    quality=\\\"balanced\\\",\\n    seed=42  # For reproducible results\\n)\\n\\n# Display the generated video\\nprint(\\\"\ud83c\udfa5 Generated Video:\\\")\\ndisplay(Video(video_path_1, width=600))\"",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example-2"
      },
      "outputs": [],
      "source": [
        "# Example 2: High quality generation (use with caution on memory)\n",
        "prompt_2 = \"A futuristic cyberpunk city at night with neon lights reflecting in rain puddles, flying cars in the distance, dramatic atmosphere\"\n",
        "\n",
        "# Check memory before high-quality generation\n",
        "mem = get_gpu_memory()\n",
        "if mem['free'] > 25:\n",
        "    print(\"\ud83d\ude80 Sufficient memory detected - proceeding with high quality\")\n",
        "    video_path_2 = generate_video_colab(\n",
        "        prompt=prompt_2,\n",
        "        quality=\"high_quality\",\n",
        "        seed=123\n",
        "    )\n",
        "    display(Video(video_path_2, width=600))\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Limited memory - using balanced quality instead\")\n",
        "    video_path_2 = generate_video_colab(\n",
        "        prompt=prompt_2,\n",
        "        quality=\"balanced\",\n",
        "        seed=123\n",
        "    )\n",
        "    display(Video(video_path_2, width=600))\n",
        "\n",
        "# Cleanup memory after generation\n",
        "cleanup_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom-generation"
      },
      "outputs": [],
      "source": [
        "# Custom generation - Enter your own prompt!\n",
        "custom_prompt = input(\"Enter your video prompt: \")\n",
        "quality_choice = input(\"Choose quality (high_quality/balanced/fast): \") or \"balanced\"\n",
        "custom_seed = input(\"Enter seed (or press enter for random): \")\n",
        "\n",
        "seed_value = None\n",
        "if custom_seed.strip():\n",
        "    try:\n",
        "        seed_value = int(custom_seed)\n",
        "    except ValueError:\n",
        "        print(\"Invalid seed, using random\")\n",
        "\n",
        "if custom_prompt.strip():\n",
        "    custom_video_path = generate_video_colab(\n",
        "        prompt=custom_prompt,\n",
        "        quality=quality_choice,\n",
        "        seed=seed_value\n",
        "    )\n",
        "    display(Video(custom_video_path, width=600))\n",
        "else:\n",
        "    print(\"No prompt entered, skipping generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch-header"
      },
      "source": [
        "## \ud83d\udcda 8. Batch Generation\n",
        "\n",
        "Generate multiple videos with automatic memory management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch-generation"
      },
      "outputs": [],
      "source": [
        "def batch_generate_videos(prompts_and_configs: list, output_dir: str = \"/content/batch_videos\"):\n",
        "    \"\"\"Generate multiple videos with automatic memory cleanup.\n",
        "\n",
        "    Args:\n",
        "        prompts_and_configs: List of (prompt, quality, seed) tuples\n",
        "        output_dir: Directory for batch outputs\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    results = []\n",
        "\n",
        "    print(f\"\ud83c\udfac Starting batch generation of {len(prompts_and_configs)} videos...\")\n",
        "\n",
        "    for i, config in enumerate(prompts_and_configs, 1):\n",
        "        prompt = config[0]\n",
        "        quality = config[1] if len(config) > 1 else \"balanced\"\n",
        "        seed = config[2] if len(config) > 2 else None\n",
        "\n",
        "        print(f\"\\n\ud83d\udcf9 Generation {i}/{len(prompts_and_configs)}:\")\n",
        "        print(f\"   Prompt: {prompt[:60]}...\" if len(prompt) > 60 else f\"   Prompt: {prompt}\")\n",
        "\n",
        "        try:\n",
        "            video_path = generate_video_colab(\n",
        "                prompt=prompt,\n",
        "                quality=quality,\n",
        "                seed=seed,\n",
        "                output_dir=output_dir\n",
        "            )\n",
        "            results.append((prompt, video_path, \"success\"))\n",
        "\n",
        "            # Cleanup memory between generations\n",
        "            if i < len(prompts_and_configs):  # Don't cleanup after last generation\n",
        "                print(\"\ud83e\uddf9 Cleaning up memory for next generation...\")\n",
        "                cleanup_memory()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Failed: {str(e)}\")\n",
        "            results.append((prompt, None, f\"error: {str(e)}\"))\n",
        "            cleanup_memory()  # Always cleanup on error\n",
        "\n",
        "    print(f\"\\n\u2705 Batch generation completed!\")\n",
        "    print(f\"\ud83d\udcca Results: {sum(1 for r in results if r[2] == 'success')}/{len(results)} successful\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example batch generation\n",
        "sample_batch = [\n",
        "    (\"A cat playing with a ball of yarn in slow motion\", \"fast\", 111),\n",
        "    (\"Ocean waves crashing on a rocky shore at dawn\", \"balanced\", 222),\n",
        "    (\"A hummingbird feeding from colorful flowers\", \"balanced\", 333)\n",
        "]\n",
        "\n",
        "print(\"\ud83c\udfac Example batch ready to run!\")\n",
        "print(\"\ud83d\udcdd Uncomment the line below to start batch generation:\")\n",
        "print(\"# batch_results = batch_generate_videos(sample_batch)\")\n",
        "\n",
        "# Uncomment to run:\n",
        "# batch_results = batch_generate_videos(sample_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export-header"
      },
      "source": [
        "## \ud83d\udcbe 9. Export & Download\n",
        "\n",
        "Export your generated videos to Google Drive or download them directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for permanent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def copy_to_drive(video_path: str, drive_folder: str = \"HunyuanVideo_Outputs\"):\n",
        "    \"\"\"Copy generated video to Google Drive.\"\"\"\n",
        "    import shutil\n",
        "\n",
        "    drive_path = f\"/content/drive/MyDrive/{drive_folder}\"\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    filename = os.path.basename(video_path)\n",
        "    drive_file = os.path.join(drive_path, filename)\n",
        "\n",
        "    shutil.copy2(video_path, drive_file)\n",
        "    print(f\"\u2705 Video copied to Google Drive: {drive_file}\")\n",
        "    return drive_file\n",
        "\n",
        "def download_video(video_path: str):\n",
        "    \"\"\"Download video to local machine.\"\"\"\n",
        "    from google.colab import files\n",
        "    if os.path.exists(video_path):\n",
        "        files.download(video_path)\n",
        "        print(f\"\u2b07\ufe0f Download started for: {os.path.basename(video_path)}\")\n",
        "    else:\n",
        "        print(f\"\u274c File not found: {video_path}\")\n",
        "\n",
        "print(\"\ud83d\udcbe Google Drive mounted successfully!\")\n",
        "print(\"\ud83d\udcc1 Use copy_to_drive(video_path) to save to Drive\")\n",
        "print(\"\u2b07\ufe0f Use download_video(video_path) to download directly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list-generated"
      },
      "outputs": [],
      "source": [
        "# List all generated videos\n",
        "import glob\n",
        "\n",
        "video_dirs = [\"/content/videos\", \"/content/batch_videos\"]\n",
        "all_videos = []\n",
        "\n",
        "for video_dir in video_dirs:\n",
        "    if os.path.exists(video_dir):\n",
        "        videos = glob.glob(os.path.join(video_dir, \"*.mp4\"))\n",
        "        all_videos.extend(videos)\n",
        "\n",
        "if all_videos:\n",
        "    print(f\"\ud83c\udfac Found {len(all_videos)} generated videos:\")\n",
        "    for i, video_path in enumerate(all_videos, 1):\n",
        "        file_size = os.path.getsize(video_path) / (1024 * 1024)\n",
        "        print(f\"   {i}. {os.path.basename(video_path)} ({file_size:.1f} MB)\")\n",
        "\n",
        "    print(\"\\n\ud83d\udcbe Export options:\")\n",
        "    print(\"   # Copy all videos to Google Drive\")\n",
        "    print(\"   for video in all_videos: copy_to_drive(video)\")\n",
        "    print(\"\")\n",
        "    print(\"   # Download specific video (replace index)\")\n",
        "    print(\"   download_video(all_videos[0])  # Downloads first video\")\n",
        "else:\n",
        "    print(\"\ud83d\udced No videos found. Generate some videos first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export-all"
      },
      "outputs": [],
      "source": [
        "# Quick export all videos to Drive\n",
        "if all_videos:\n",
        "    export_choice = input(\"Export all videos to Google Drive? (y/n): \")\n",
        "    if export_choice.lower() == 'y':\n",
        "        print(\"\ud83d\udce4 Exporting all videos to Google Drive...\")\n",
        "        for video_path in all_videos:\n",
        "            try:\n",
        "                copy_to_drive(video_path)\n",
        "            except Exception as e:\n",
        "                print(f\"\u274c Failed to copy {video_path}: {e}\")\n",
        "        print(\"\u2705 Export completed!\")\n",
        "    else:\n",
        "        print(\"\ud83d\udccb Export skipped. Use copy_to_drive(video_path) for individual files.\")\n",
        "else:\n",
        "    print(\"\ud83d\udced No videos to export.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting-header"
      },
      "source": [
        "## \ud83d\udd27 10. Troubleshooting & Optimization\n",
        "\n",
        "Common issues and solutions for HunyuanVideo on Colab A100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diagnostics"
      },
      "outputs": [],
      "source": [
        "def run_diagnostics():\n",
        "    \"\"\"Run comprehensive system diagnostics.\"\"\"\n",
        "    print(\"\ud83d\udd0d Running HunyuanVideo Colab Diagnostics...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # GPU Information\n",
        "    print(\"\\n\ud83d\udda5\ufe0f GPU Information:\")\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_props = torch.cuda.get_device_properties(0)\n",
        "        print(f\"   GPU: {gpu_props.name}\")\n",
        "        print(f\"   Total Memory: {gpu_props.total_memory / 1024**3:.1f} GB\")\n",
        "        print(f\"   CUDA Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
        "\n",
        "        # Check if A100\n",
        "        if \"A100\" in gpu_props.name:\n",
        "            print(\"   \u2705 A100 GPU detected - optimal for HunyuanVideo\")\n",
        "        else:\n",
        "            print(\"   \u26a0\ufe0f Non-A100 GPU - may need optimization\")\n",
        "    else:\n",
        "        print(\"   \u274c No GPU available\")\n",
        "\n",
        "    # Memory Status\n",
        "    print_memory_status(\"\\n\ud83e\udde0 Current Memory\")\n",
        "\n",
        "    # Disk Space\n",
        "    print(\"\\n\ud83d\udcbe Disk Space:\")\n",
        "    import shutil\n",
        "    total, used, free = shutil.disk_usage(\"/content\")\n",
        "    print(f\"   Total: {total / 1024**3:.1f} GB\")\n",
        "    print(f\"   Used: {used / 1024**3:.1f} GB\")\n",
        "    print(f\"   Free: {free / 1024**3:.1f} GB\")\n",
        "\n",
        "    if free / 1024**3 < 20:\n",
        "        print(\"   \u26a0\ufe0f Low disk space - may affect model loading\")\n",
        "\n",
        "    # Library Versions\n",
        "    print(\"\\n\ud83d\udcda Library Versions:\")\n",
        "    try:\n",
        "        import diffusers, transformers, torch\n",
        "        print(f\"   PyTorch: {torch.__version__}\")\n",
        "        print(f\"   Diffusers: {diffusers.__version__}\")\n",
        "        print(f\"   Transformers: {transformers.__version__}\")\n",
        "\n",
        "        # Check versions\n",
        "        if diffusers.__version__ >= \"0.33.1\":\n",
        "            print(\"   \u2705 Diffusers version supports HunyuanVideo\")\n",
        "        else:\n",
        "            print(\"   \u26a0\ufe0f Update diffusers: pip install diffusers>=0.33.1\")\n",
        "    except ImportError as e:\n",
        "        print(f\"   \u274c Import error: {e}\")\n",
        "\n",
        "    # Model Status\n",
        "    print(\"\\n\ud83e\udd16 Model Status:\")\n",
        "    try:\n",
        "        # Check if pipe is loaded\n",
        "        if 'pipe' in globals():\n",
        "            print(\"   \u2705 HunyuanVideo pipeline loaded\")\n",
        "            print(f\"   Device: {next(pipe.parameters()).device}\")\n",
        "            print(f\"   Dtype: {next(pipe.parameters()).dtype}\")\n",
        "        else:\n",
        "            print(\"   \u274c HunyuanVideo pipeline not loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"   \u26a0\ufe0f Error checking model: {e}\")\n",
        "\n",
        "    print(\"\\n\ud83d\udd27 Optimization Status:\")\n",
        "    print(f\"   TF32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")\n",
        "    print(f\"   cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
        "    print(f\"   Memory fraction: {torch.cuda.get_memory_fraction(0) if torch.cuda.is_available() else 'N/A'}\")\n",
        "\n",
        "def memory_optimization_tips():\n",
        "    \"\"\"Show memory optimization tips.\"\"\"\n",
        "    print(\"\ud83d\udca1 Memory Optimization Tips:\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"\\n\ud83d\udd27 If you get Out of Memory errors:\")\n",
        "    print(\"   1. Use 'fast' quality setting\")\n",
        "    print(\"   2. Reduce num_frames (try 32 instead of 65)\")\n",
        "    print(\"   3. Lower resolution (512x512 instead of 720p)\")\n",
        "    print(\"   4. Restart runtime to clear memory\")\n",
        "    print(\"   5. Run cleanup_memory() between generations\")\n",
        "\n",
        "    print(\"\\n\u26a1 For better performance:\")\n",
        "    print(\"   1. Use sequential_cpu_offload (already enabled)\")\n",
        "    print(\"   2. Enable VAE tiling (already enabled)\")\n",
        "    print(\"   3. Use FP16 precision (already enabled)\")\n",
        "    print(\"   4. Generate videos one at a time\")\n",
        "    print(\"   5. Close browser tabs to free system RAM\")\n",
        "\n",
        "# Run diagnostics\n",
        "run_diagnostics()\n",
        "memory_optimization_tips()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emergency-cleanup"
      },
      "outputs": [],
      "source": [
        "# Emergency memory cleanup\n",
        "def emergency_cleanup():\n",
        "    \"\"\"Aggressive memory cleanup for emergency situations.\"\"\"\n",
        "    print(\"\ud83d\udea8 Running emergency memory cleanup...\")\n",
        "\n",
        "    # Clear Python variables\n",
        "    if 'pipe' in globals():\n",
        "        print(\"   Clearing pipeline...\")\n",
        "        del pipe\n",
        "\n",
        "    # Garbage collection\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    # CUDA cleanup\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "        print(\"   CUDA cache cleared\")\n",
        "\n",
        "    # Check memory after cleanup\n",
        "    print_memory_status(\"(After cleanup)\")\n",
        "\n",
        "    print(\"\u2705 Emergency cleanup completed\")\n",
        "    print(\"\ud83d\udca1 You may need to reload the model with the loading cell above\")\n",
        "\n",
        "print(\"\ud83d\udea8 Emergency cleanup function ready\")\n",
        "print(\"\ud83d\udcdd Run emergency_cleanup() if you encounter memory issues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion-header"
      },
      "source": [
        "## \ud83c\udf89 Conclusion\n",
        "\n",
        "Congratulations! You've successfully set up and used HunyuanVideo on Google Colab A100.\n",
        "\n",
        "### \ud83c\udfaf What You've Accomplished:\n",
        "- \u2705 Loaded a 13B parameter video generation model on Colab\n",
        "- \u2705 Optimized for A100 40GB memory constraints\n",
        "- \u2705 Generated high-quality videos up to 15 seconds\n",
        "- \u2705 Learned memory management for large AI models\n",
        "- \u2705 Set up batch generation and export workflows\n",
        "\n",
        "### \ud83d\ude80 Next Steps:\n",
        "1. **Experiment** with different prompts and quality settings\n",
        "2. **Explore** the full videogenbook library for more models\n",
        "3. **Read** the complete guide: *Hands-On Video Generation with AI*\n",
        "4. **Join** the community: [GitHub Repository](https://github.com/jenochs/video-generation-book)\n",
        "\n",
        "### \ud83d\udd17 Resources:\n",
        "- **Documentation**: [videogenbook docs](https://github.com/jenochs/video-generation-book)\n",
        "- **HunyuanVideo**: [Model Card](https://huggingface.co/tencent/HunyuanVideo)\n",
        "- **Colab Pro+**: [Upgrade for A100 access](https://colab.research.google.com/signup)\n",
        "\n",
        "### \ud83d\udca1 Tips for Best Results:\n",
        "- Use detailed, descriptive prompts\n",
        "- Include camera movement descriptions\n",
        "- Specify lighting and atmosphere\n",
        "- Experiment with different seeds for variety\n",
        "- Monitor memory usage for stable generation\n",
        "\n",
        "**Happy video generating! \ud83c\udfac\u2728**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}