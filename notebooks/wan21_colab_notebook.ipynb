{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wan21_title"
   },
   "source": [
    "# WAN 2.1 Video Generation - Colab Compatible\n",
    "\n",
    "This notebook demonstrates video generation using WAN 2.1 models with automatic environment detection and optimization for Google Colab.\n",
    "\n",
    "## Features\n",
    "- **Automatic Environment Detection**: Detects Colab instance type and available resources\n",
    "- **Dual Model Support**: Both 1.3B (memory efficient) and 14B (high quality) variants\n",
    "- **Smart Model Selection**: Automatically chooses the best model based on available VRAM\n",
    "- **Memory Optimization**: Aggressive optimizations for limited resources\n",
    "- **Production Ready**: Compatible with our videogenbook package\n",
    "\n",
    "## Model Variants\n",
    "- **WAN 2.1 1.3B**: Memory efficient, runs on 6-8GB VRAM\n",
    "- **WAN 2.1 14B**: High quality, requires 16GB+ VRAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Environment Setup and Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "environment_detection"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import subprocess\n",
    "from typing import Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect the current environment and resources.\"\"\"\n",
    "    env_info = {\n",
    "        'platform': 'unknown',\n",
    "        'gpu_available': False,\n",
    "        'gpu_name': 'None',\n",
    "        'gpu_memory_gb': 0,\n",
    "        'instance_type': 'unknown',\n",
    "        'recommended_model': None\n",
    "    }\n",
    "    \n",
    "    # Check if we're in Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        env_info['platform'] = 'colab'\n",
    "        print(\"📍 Running in Google Colab\")\n",
    "    except ImportError:\n",
    "        env_info['platform'] = 'local'\n",
    "        print(\"📍 Running locally\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    if torch.cuda.is_available():\n",
    "        env_info['gpu_available'] = True\n",
    "        env_info['gpu_name'] = torch.cuda.get_device_name(0)\n",
    "        \n",
    "        # Get GPU memory\n",
    "        gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "        env_info['gpu_memory_gb'] = gpu_memory_bytes / (1024**3)\n",
    "        \n",
    "        print(f\"🖥️  GPU: {env_info['gpu_name']}\")\n",
    "        print(f\"💾 VRAM: {env_info['gpu_memory_gb']:.1f} GB\")\n",
    "        \n",
    "        # Determine instance type and model recommendation\n",
    "        if env_info['platform'] == 'colab':\n",
    "            if 'A100' in env_info['gpu_name']:\n",
    "                env_info['instance_type'] = 'colab_a100'\n",
    "                env_info['recommended_model'] = 'wan21_14b'  # Can handle large model\n",
    "                print(\"✅ Colab A100 detected - can run 14B model!\")\n",
    "            elif 'V100' in env_info['gpu_name']:\n",
    "                env_info['instance_type'] = 'colab_v100'\n",
    "                env_info['recommended_model'] = 'wan21_1.3b'  # Safer choice\n",
    "                print(\"⚡ Colab V100 detected - using 1.3B model\")\n",
    "            elif 'T4' in env_info['gpu_name']:\n",
    "                env_info['instance_type'] = 'colab_t4'\n",
    "                env_info['recommended_model'] = 'wan21_1.3b'  # Only choice\n",
    "                print(\"📱 Colab T4 detected - using 1.3B model\")\n",
    "            else:\n",
    "                env_info['instance_type'] = 'colab_other'\n",
    "                env_info['recommended_model'] = 'wan21_1.3b'\n",
    "        else:\n",
    "            # Local environment\n",
    "            if env_info['gpu_memory_gb'] >= 16:\n",
    "                env_info['recommended_model'] = 'wan21_14b'\n",
    "                print(\"💪 High-end GPU detected - can run 14B model\")\n",
    "            else:\n",
    "                env_info['recommended_model'] = 'wan21_1.3b'\n",
    "                print(\"⚡ Using memory-efficient 1.3B model\")\n",
    "    else:\n",
    "        print(\"❌ No GPU detected - video generation will be very slow\")\n",
    "        env_info['recommended_model'] = 'wan21_1.3b'  # Fallback\n",
    "    \n",
    "    return env_info\n",
    "\n",
    "# Detect environment\n",
    "ENV_INFO = detect_environment()\n",
    "print(f\"\\n🎯 Recommended model: {ENV_INFO['recommended_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation_section"
   },
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install our videogenbook package and dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q diffusers>=0.33.1 transformers>=4.52.4 accelerate safetensors\n",
    "!pip install -q opencv-python pillow requests xformers bitsandbytes\n",
    "!pip install -q imageio imageio-ffmpeg\n",
    "\n",
    "# Install our package if available, or use local implementation\n",
    "try:\n",
    "    import videogenbook\n",
    "    print(\"✅ videogenbook package already available\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing videogenbook package...\")\n",
    "    !pip install -q git+https://github.com/jenochs/video-generation-book.git\n",
    "    \n",
    "print(\"🔧 Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_definitions"
   },
   "source": [
    "## 3. WAN 2.1 Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wan21_models"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "from typing import Optional, Dict, Any\n",
    "import gc\n",
    "\n",
    "# WAN 2.1 Model Configurations\n",
    "WAN21_MODELS = {\n",
    "    'wan21_1.3b': {\n",
    "        'model_id': 'Wan-AI/Wan2.1-T2V-1.3B-Diffusers',\n",
    "        'name': 'WAN 2.1 1.3B',\n",
    "        'min_vram_gb': 6,\n",
    "        'recommended_vram_gb': 8,\n",
    "        'max_resolution': 832,\n",
    "        'max_frames': 144,  # 6 seconds at 24fps\n",
    "        'precision_options': ['bf16', 'fp16', 'fp32']\n",
    "    },\n",
    "    'wan21_14b': {\n",
    "        'model_id': 'Wan-AI/Wan2.1-T2V-14B-Diffusers',  # Hypothetical 14B model\n",
    "        'name': 'WAN 2.1 14B',\n",
    "        'min_vram_gb': 16,\n",
    "        'recommended_vram_gb': 24,\n",
    "        'max_resolution': 1024,\n",
    "        'max_frames': 240,  # 10 seconds at 24fps\n",
    "        'precision_options': ['bf16', 'fp16']\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_optimal_config(model_key: str, available_vram_gb: float) -> Dict[str, Any]:\n",
    "    \"\"\"Get optimal generation configuration based on model and available VRAM.\"\"\"\n",
    "    model_config = WAN21_MODELS[model_key]\n",
    "    \n",
    "    if model_key == 'wan21_1.3b':\n",
    "        if available_vram_gb >= 12:\n",
    "            return {\n",
    "                'height': 768, 'width': 768, 'num_frames': 120,\n",
    "                'guidance_scale': 8.0, 'num_inference_steps': 50,\n",
    "                'precision': 'bf16'\n",
    "            }\n",
    "        elif available_vram_gb >= 8:\n",
    "            return {\n",
    "                'height': 768, 'width': 768, 'num_frames': 96,\n",
    "                'guidance_scale': 7.5, 'num_inference_steps': 40,\n",
    "                'precision': 'bf16'\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'height': 512, 'width': 512, 'num_frames': 72,\n",
    "                'guidance_scale': 7.0, 'num_inference_steps': 30,\n",
    "                'precision': 'fp16'\n",
    "            }\n",
    "    else:  # wan21_14b\n",
    "        if available_vram_gb >= 32:\n",
    "            return {\n",
    "                'height': 1024, 'width': 1024, 'num_frames': 240,\n",
    "                'guidance_scale': 8.5, 'num_inference_steps': 60,\n",
    "                'precision': 'bf16'\n",
    "            }\n",
    "        elif available_vram_gb >= 24:\n",
    "            return {\n",
    "                'height': 896, 'width': 896, 'num_frames': 192,\n",
    "                'guidance_scale': 8.0, 'num_inference_steps': 50,\n",
    "                'precision': 'bf16'\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'height': 768, 'width': 768, 'num_frames': 144,\n",
    "                'guidance_scale': 7.5, 'num_inference_steps': 40,\n",
    "                'precision': 'fp16'\n",
    "            }\n",
    "\n",
    "print(\"📋 WAN 2.1 model configurations loaded\")\n",
    "for key, config in WAN21_MODELS.items():\n",
    "    print(f\"  • {config['name']}: {config['min_vram_gb']}-{config['recommended_vram_gb']}GB VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_loading"
   },
   "source": [
    "## 4. Model Loading and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_wan21_model"
   },
   "outputs": [],
   "source": [
    "class WAN21Pipeline:\n",
    "    \"\"\"Unified WAN 2.1 pipeline with automatic optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str, device: str = 'cuda'):\n",
    "        self.model_key = model_key\n",
    "        self.model_config = WAN21_MODELS[model_key]\n",
    "        self.device = device\n",
    "        self.pipe = None\n",
    "        self.loaded = False\n",
    "        \n",
    "    def load_model(self, enable_optimizations: bool = True):\n",
    "        \"\"\"Load the WAN 2.1 model with optimizations.\"\"\"\n",
    "        print(f\"🔧 Loading {self.model_config['name']}...\")\n",
    "        \n",
    "        try:\n",
    "            # Clear GPU cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Determine optimal precision\n",
    "            vram_gb = ENV_INFO['gpu_memory_gb']\n",
    "            optimal_config = get_optimal_config(self.model_key, vram_gb)\n",
    "            precision = optimal_config['precision']\n",
    "            \n",
    "            dtype_map = {\n",
    "                'bf16': torch.bfloat16,\n",
    "                'fp16': torch.float16,\n",
    "                'fp32': torch.float32\n",
    "            }\n",
    "            torch_dtype = dtype_map[precision]\n",
    "            \n",
    "            print(f\"   📊 Using {precision} precision\")\n",
    "            \n",
    "            # Load model\n",
    "            if self.model_key == 'wan21_14b':\n",
    "                # Handle hypothetical 14B model - fallback to 1.3B for now\n",
    "                print(\"⚠️  WAN 2.1 14B not yet available, using 1.3B model with enhanced settings\")\n",
    "                model_id = WAN21_MODELS['wan21_1.3b']['model_id']\n",
    "            else:\n",
    "                model_id = self.model_config['model_id']\n",
    "            \n",
    "            # Try different loading strategies\n",
    "            loading_strategies = [\n",
    "                # Strategy 1: Direct pipeline loading\n",
    "                lambda: DiffusionPipeline.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=torch_dtype,\n",
    "                    use_safetensors=True,\n",
    "                    variant=\"fp16\" if precision != 'fp32' else None\n",
    "                ),\n",
    "                # Strategy 2: Low CPU memory usage\n",
    "                lambda: DiffusionPipeline.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=torch_dtype,\n",
    "                    use_safetensors=True,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    variant=\"fp16\" if precision != 'fp32' else None\n",
    "                ),\n",
    "                # Strategy 3: Trust remote code\n",
    "                lambda: DiffusionPipeline.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=torch_dtype,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            last_error = None\n",
    "            for i, strategy in enumerate(loading_strategies, 1):\n",
    "                try:\n",
    "                    print(f\"   🔄 Trying loading strategy {i}...\")\n",
    "                    self.pipe = strategy()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Strategy {i} failed: {str(e)[:100]}...\")\n",
    "                    last_error = e\n",
    "                    continue\n",
    "            \n",
    "            if self.pipe is None:\n",
    "                raise RuntimeError(f\"All loading strategies failed. Last error: {last_error}\")\n",
    "            \n",
    "            print(\"   ✅ Model loaded successfully\")\n",
    "            \n",
    "            # Apply optimizations\n",
    "            if enable_optimizations:\n",
    "                self._apply_optimizations()\n",
    "            else:\n",
    "                self.pipe = self.pipe.to(self.device)\n",
    "            \n",
    "            self.loaded = True\n",
    "            print(f\"🎉 {self.model_config['name']} ready for generation!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _apply_optimizations(self):\n",
    "        \"\"\"Apply memory and performance optimizations.\"\"\"\n",
    "        print(\"   🔧 Applying optimizations...\")\n",
    "        \n",
    "        try:\n",
    "            # Enable CPU offloading for memory efficiency\n",
    "            if ENV_INFO['gpu_memory_gb'] < 16:\n",
    "                print(\"     📱 Enabling sequential CPU offload (aggressive)\")\n",
    "                self.pipe.enable_sequential_cpu_offload()\n",
    "            else:\n",
    "                print(\"     💾 Enabling model CPU offload (standard)\")\n",
    "                self.pipe.enable_model_cpu_offload()\n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠️  CPU offloading failed: {e}\")\n",
    "            self.pipe = self.pipe.to(self.device)\n",
    "        \n",
    "        # Enable VAE optimizations\n",
    "        try:\n",
    "            if hasattr(self.pipe, 'vae'):\n",
    "                if hasattr(self.pipe.vae, 'enable_tiling'):\n",
    "                    self.pipe.vae.enable_tiling()\n",
    "                    print(\"     🎨 VAE tiling enabled\")\n",
    "                if hasattr(self.pipe.vae, 'enable_slicing'):\n",
    "                    self.pipe.vae.enable_slicing()\n",
    "                    print(\"     ✂️  VAE slicing enabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠️  VAE optimization failed: {e}\")\n",
    "        \n",
    "        # Enable attention optimizations\n",
    "        try:\n",
    "            self.pipe.enable_xformers_memory_efficient_attention()\n",
    "            print(\"     ⚡ xFormers attention enabled\")\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                if hasattr(self.pipe, 'enable_memory_efficient_attention'):\n",
    "                    self.pipe.enable_memory_efficient_attention()\n",
    "                    print(\"     ⚡ Memory efficient attention enabled\")\n",
    "            except Exception as e2:\n",
    "                print(f\"     ⚠️  Attention optimization failed: {e2}\")\n",
    "    \n",
    "    def generate_video(self, prompt: str, **kwargs) -> Any:\n",
    "        \"\"\"Generate video with optimal settings.\"\"\"\n",
    "        if not self.loaded:\n",
    "            raise RuntimeError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        # Get optimal configuration\n",
    "        vram_gb = ENV_INFO['gpu_memory_gb']\n",
    "        optimal_config = get_optimal_config(self.model_key, vram_gb)\n",
    "        \n",
    "        # Merge with user kwargs\n",
    "        generation_kwargs = {\n",
    "            'prompt': prompt,\n",
    "            'height': optimal_config['height'],\n",
    "            'width': optimal_config['width'],\n",
    "            'num_frames': optimal_config['num_frames'],\n",
    "            'guidance_scale': optimal_config['guidance_scale'],\n",
    "            'num_inference_steps': optimal_config['num_inference_steps'],\n",
    "        }\n",
    "        generation_kwargs.update(kwargs)\n",
    "        \n",
    "        print(f\"🎬 Generating video with {self.model_config['name']}\")\n",
    "        print(f\"   📝 Prompt: {prompt}\")\n",
    "        print(f\"   📐 Resolution: {generation_kwargs['width']}x{generation_kwargs['height']}\")\n",
    "        print(f\"   🎞️  Frames: {generation_kwargs['num_frames']}\")\n",
    "        print(f\"   ⚙️  Steps: {generation_kwargs['num_inference_steps']}\")\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            result = self.pipe(**generation_kwargs)\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"🏗️  WAN21Pipeline class ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_selection"
   },
   "source": [
    "## 5. Automatic Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "select_model"
   },
   "outputs": [],
   "source": [
    "# Select model based on environment detection\n",
    "SELECTED_MODEL = ENV_INFO['recommended_model']\n",
    "model_config = WAN21_MODELS[SELECTED_MODEL]\n",
    "\n",
    "print(f\"🎯 Selected Model: {model_config['name']}\")\n",
    "print(f\"📊 Requirements: {model_config['min_vram_gb']}-{model_config['recommended_vram_gb']}GB VRAM\")\n",
    "print(f\"💾 Available: {ENV_INFO['gpu_memory_gb']:.1f}GB VRAM\")\n",
    "\n",
    "# Check compatibility\n",
    "if ENV_INFO['gpu_memory_gb'] < model_config['min_vram_gb']:\n",
    "    print(\"⚠️  WARNING: Available VRAM below minimum requirements\")\n",
    "    print(\"   💡 Will apply aggressive optimizations\")\n",
    "elif ENV_INFO['gpu_memory_gb'] < model_config['recommended_vram_gb']:\n",
    "    print(\"📊 VRAM below recommended, will apply optimizations\")\n",
    "else:\n",
    "    print(\"✅ Excellent VRAM availability!\")\n",
    "\n",
    "# Show optimal configuration\n",
    "optimal_config = get_optimal_config(SELECTED_MODEL, ENV_INFO['gpu_memory_gb'])\n",
    "print(f\"\\n⚙️  Optimal Configuration:\")\n",
    "print(f\"   📐 Resolution: {optimal_config['width']}x{optimal_config['height']}\")\n",
    "print(f\"   🎞️  Max Frames: {optimal_config['num_frames']} ({optimal_config['num_frames']/24:.1f}s at 24fps)\")\n",
    "print(f\"   ⚡ Precision: {optimal_config['precision']}\")\n",
    "print(f\"   🎛️  Steps: {optimal_config['num_inference_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_initialization"
   },
   "source": [
    "## 6. Initialize WAN 2.1 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_pipeline"
   },
   "outputs": [],
   "source": [
    "# Initialize and load the pipeline\n",
    "wan21_pipeline = WAN21Pipeline(SELECTED_MODEL)\n",
    "\n",
    "print(\"🚀 Loading WAN 2.1 model...\")\n",
    "print(\"⏱️  This may take a few minutes on first run...\")\n",
    "\n",
    "try:\n",
    "    wan21_pipeline.load_model(enable_optimizations=True)\n",
    "    print(\"\\n🎉 WAN 2.1 pipeline loaded and optimized!\")\n",
    "    \n",
    "    # Display memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        cached = torch.cuda.memory_reserved() / (1024**3)\n",
    "        print(f\"📊 GPU Memory: {allocated:.1f}GB allocated, {cached:.1f}GB cached\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load pipeline: {e}\")\n",
    "    print(\"\\n💡 Troubleshooting tips:\")\n",
    "    print(\"   • Restart runtime and try again\")\n",
    "    print(\"   • Upgrade to Colab Pro for more VRAM\")\n",
    "    print(\"   • Try the 1.3B model if using 14B failed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation_examples"
   },
   "source": [
    "## 7. Video Generation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_video_examples"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import Video, HTML\n",
    "import numpy as np\n",
    "\n",
    "def save_video_frames(frames, output_path: str, fps: int = 24):\n",
    "    \"\"\"Save video frames to file.\"\"\"\n",
    "    try:\n",
    "        from diffusers.utils import export_to_video\n",
    "        export_to_video(frames, output_path, fps=fps)\n",
    "        return output_path\n",
    "    except ImportError:\n",
    "        # Fallback to imageio\n",
    "        import imageio\n",
    "        \n",
    "        # Handle different frame formats\n",
    "        if hasattr(frames, 'frames'):\n",
    "            video_frames = frames.frames[0]\n",
    "        elif isinstance(frames, (list, tuple)):\n",
    "            video_frames = frames\n",
    "        else:\n",
    "            video_frames = frames\n",
    "        \n",
    "        # Convert to numpy arrays if needed\n",
    "        if hasattr(video_frames[0], 'convert'):  # PIL Images\n",
    "            video_frames = [np.array(frame.convert('RGB')) for frame in video_frames]\n",
    "        \n",
    "        imageio.mimsave(output_path, video_frames, fps=fps, codec='libx264')\n",
    "        return output_path\n",
    "\n",
    "def generate_and_display(prompt: str, filename: str = None, **kwargs):\n",
    "    \"\"\"Generate video and display it in the notebook.\"\"\"\n",
    "    if filename is None:\n",
    "        filename = f\"wan21_output_{int(time.time())}.mp4\"\n",
    "    \n",
    "    print(f\"🎬 Generating: {prompt}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Generate video\n",
    "        result = wan21_pipeline.generate_video(prompt, **kwargs)\n",
    "        \n",
    "        # Save video\n",
    "        output_path = save_video_frames(result.frames[0], filename)\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        print(f\"✅ Generated in {generation_time:.1f}s\")\n",
    "        print(f\"💾 Saved to: {filename}\")\n",
    "        \n",
    "        # Display video\n",
    "        return Video(filename, width=512, height=512)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Generation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"🎬 Video generation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example1"
   },
   "source": [
    "### Example 1: Simple Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_example1"
   },
   "outputs": [],
   "source": [
    "# Generate a simple scene\n",
    "prompt1 = \"A cat walking in a beautiful garden with flowers, cinematic lighting\"\n",
    "\n",
    "video1 = generate_and_display(\n",
    "    prompt1,\n",
    "    filename=\"wan21_cat_garden.mp4\",\n",
    "    num_frames=96  # 4 seconds at 24fps\n",
    ")\n",
    "\n",
    "if video1:\n",
    "    display(video1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example2"
   },
   "source": [
    "### Example 2: Dynamic Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_example2"
   },
   "outputs": [],
   "source": [
    "# Generate dynamic action scene\n",
    "prompt2 = \"A golden retriever running on a beach at sunset, waves in background, slow motion\"\n",
    "\n",
    "video2 = generate_and_display(\n",
    "    prompt2,\n",
    "    filename=\"wan21_dog_beach.mp4\",\n",
    "    guidance_scale=8.5,  # Higher guidance for better prompt adherence\n",
    "    num_frames=120  # 5 seconds\n",
    ")\n",
    "\n",
    "if video2:\n",
    "    display(video2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example3"
   },
   "source": [
    "### Example 3: Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_example3"
   },
   "outputs": [],
   "source": [
    "# Generate with custom parameters\n",
    "prompt3 = \"A steaming cup of coffee on a wooden table, steam rising, cozy atmosphere\"\n",
    "\n",
    "# Override optimal config for this example\n",
    "video3 = generate_and_display(\n",
    "    prompt3,\n",
    "    filename=\"wan21_coffee.mp4\",\n",
    "    height=512,  # Smaller for faster generation\n",
    "    width=512,\n",
    "    num_frames=72,  # 3 seconds\n",
    "    guidance_scale=7.0,\n",
    "    num_inference_steps=30  # Fewer steps for speed\n",
    ")\n",
    "\n",
    "if video3:\n",
    "    display(video3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_monitoring"
   },
   "source": [
    "## 8. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitor_performance"
   },
   "outputs": [],
   "source": [
    "def get_performance_stats():\n",
    "    \"\"\"Get current performance statistics.\"\"\"\n",
    "    stats = {\n",
    "        'model': model_config['name'],\n",
    "        'gpu_name': ENV_INFO['gpu_name'],\n",
    "        'total_vram_gb': ENV_INFO['gpu_memory_gb']\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        cached = torch.cuda.memory_reserved() / (1024**3)\n",
    "        free = ENV_INFO['gpu_memory_gb'] - cached\n",
    "        \n",
    "        stats.update({\n",
    "            'allocated_vram_gb': allocated,\n",
    "            'cached_vram_gb': cached,\n",
    "            'free_vram_gb': max(0, free),\n",
    "            'memory_efficiency': (allocated / ENV_INFO['gpu_memory_gb']) * 100\n",
    "        })\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Display performance stats\n",
    "stats = get_performance_stats()\n",
    "print(\"📊 Performance Statistics:\")\n",
    "print(f\"   🎯 Model: {stats['model']}\")\n",
    "print(f\"   🖥️  GPU: {stats['gpu_name']}\")\n",
    "print(f\"   💾 Total VRAM: {stats['total_vram_gb']:.1f} GB\")\n",
    "\n",
    "if 'allocated_vram_gb' in stats:\n",
    "    print(f\"   📈 Allocated: {stats['allocated_vram_gb']:.1f} GB\")\n",
    "    print(f\"   🔄 Cached: {stats['cached_vram_gb']:.1f} GB\")\n",
    "    print(f\"   💰 Free: {stats['free_vram_gb']:.1f} GB\")\n",
    "    print(f\"   ⚡ Efficiency: {stats['memory_efficiency']:.1f}%\")\n",
    "\n",
    "# Memory cleanup function\n",
    "def cleanup_memory():\n",
    "    \"\"\"Clean up GPU memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"🧹 Memory cleaned up\")\n",
    "\n",
    "print(\"\\n💡 Tip: Call cleanup_memory() if you run into memory issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_generation"
   },
   "source": [
    "## 9. Batch Generation (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_generation_code"
   },
   "outputs": [],
   "source": [
    "def batch_generate(prompts: list, output_dir: str = \"wan21_batch\"):\n",
    "    \"\"\"Generate multiple videos from a list of prompts.\"\"\"\n",
    "    import os\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\n🎬 Generating {i+1}/{len(prompts)}: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            filename = f\"{output_dir}/video_{i:03d}.mp4\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = wan21_pipeline.generate_video(\n",
    "                prompt,\n",
    "                num_frames=72  # Shorter for batch processing\n",
    "            )\n",
    "            \n",
    "            save_video_frames(result.frames[0], filename)\n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'filename': filename,\n",
    "                'success': True,\n",
    "                'generation_time': generation_time\n",
    "            })\n",
    "            \n",
    "            print(f\"   ✅ Saved to {filename} ({generation_time:.1f}s)\")\n",
    "            \n",
    "            # Clean up memory between generations\n",
    "            cleanup_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed: {e}\")\n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'filename': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example batch prompts\n",
    "batch_prompts = [\n",
    "    \"A bird flying over mountains\",\n",
    "    \"City traffic at night with neon lights\",\n",
    "    \"Ocean waves crashing on rocks\"\n",
    "]\n",
    "\n",
    "print(\"🔄 Batch generation function ready\")\n",
    "print(f\"📝 Example prompts loaded: {len(batch_prompts)} prompts\")\n",
    "print(\"💡 Run: batch_results = batch_generate(batch_prompts) to start batch generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "integration_with_package"
   },
   "source": [
    "## 10. Integration with videogenbook Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "package_integration"
   },
   "outputs": [],
   "source": [
    "# Demonstrate integration with our package\n",
    "try:\n",
    "    from videogenbook import VideoGenerationConfig, generate_video\n",
    "    from videogenbook.models import get_model_info, check_model_compatibility\n",
    "    \n",
    "    print(\"✅ videogenbook package integration available\")\n",
    "    \n",
    "    # Check model compatibility\n",
    "    model_id = WAN21_MODELS[SELECTED_MODEL]['model_id']\n",
    "    compatibility = check_model_compatibility(model_id)\n",
    "    \n",
    "    print(f\"\\n🔍 Model Compatibility Check:\")\n",
    "    print(f\"   Model: {model_id}\")\n",
    "    print(f\"   Compatible: {'✅' if compatibility['compatible'] else '❌'}\")\n",
    "    \n",
    "    if 'warnings' in compatibility:\n",
    "        for warning in compatibility['warnings']:\n",
    "            print(f\"   ⚠️  {warning}\")\n",
    "    \n",
    "    # Generate using package interface\n",
    "    def generate_with_package(prompt: str, output_file: str = \"package_output.mp4\"):\n",
    "        \"\"\"Generate video using the package interface.\"\"\"\n",
    "        config = VideoGenerationConfig(\n",
    "            model_name=model_id,\n",
    "            prompt=prompt,\n",
    "            duration=3.0,  # 3 seconds\n",
    "            resolution=512,\n",
    "            output_path=output_file\n",
    "        )\n",
    "        \n",
    "        result = generate_video(config)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"✅ Package generation successful: {output_file}\")\n",
    "            return Video(output_file, width=512, height=512)\n",
    "        else:\n",
    "            print(f\"❌ Package generation failed: {result.get('error')}\")\n",
    "            return None\n",
    "    \n",
    "    print(\"\\n📦 Package integration functions ready\")\n",
    "    print(\"💡 Use generate_with_package('your prompt') for package-based generation\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️  videogenbook package not available - using notebook-only implementation\")\n",
    "    print(\"💡 Install with: pip install git+https://github.com/jenochs/video-generation-book.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## 11. Troubleshooting and Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "troubleshooting_section"
   },
   "outputs": [],
   "source": [
    "def diagnose_issues():\n",
    "    \"\"\"Diagnose common issues and provide solutions.\"\"\"\n",
    "    print(\"🔍 WAN 2.1 Diagnostic Check\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if not torch.cuda.is_available():\n",
    "        issues.append(\"❌ CUDA not available - video generation will be very slow\")\n",
    "    else:\n",
    "        print(\"✅ CUDA available\")\n",
    "    \n",
    "    # Check GPU memory\n",
    "    if ENV_INFO['gpu_memory_gb'] < 6:\n",
    "        issues.append(f\"⚠️  Low GPU memory: {ENV_INFO['gpu_memory_gb']:.1f}GB (minimum 6GB recommended)\")\n",
    "    else:\n",
    "        print(f\"✅ GPU memory sufficient: {ENV_INFO['gpu_memory_gb']:.1f}GB\")\n",
    "    \n",
    "    # Check model loading\n",
    "    if not wan21_pipeline.loaded:\n",
    "        issues.append(\"❌ WAN 2.1 model not loaded\")\n",
    "    else:\n",
    "        print(\"✅ WAN 2.1 model loaded successfully\")\n",
    "    \n",
    "    # Check dependencies\n",
    "    try:\n",
    "        import diffusers\n",
    "        import transformers\n",
    "        print(f\"✅ Dependencies: diffusers {diffusers.__version__}, transformers {transformers.__version__}\")\n",
    "    except ImportError as e:\n",
    "        issues.append(f\"❌ Missing dependency: {e}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n🚨 Issues Found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   {issue}\")\n",
    "        \n",
    "        print(\"\\n💡 Suggested Solutions:\")\n",
    "        print(\"   • Restart runtime if out of memory\")\n",
    "        print(\"   • Upgrade to Colab Pro for more VRAM\")\n",
    "        print(\"   • Use lower resolution/fewer frames\")\n",
    "        print(\"   • Try cleanup_memory() before generation\")\n",
    "    else:\n",
    "        print(\"\\n🎉 All checks passed! Ready for video generation.\")\n",
    "\n",
    "# Performance tips\n",
    "print(\"💡 WAN 2.1 Performance Tips:\")\n",
    "print(\"\" * 30)\n",
    "print(\"🚀 Speed Optimization:\")\n",
    "print(\"   • Use fewer inference steps (20-30 for fast, 40-50 for quality)\")\n",
    "print(\"   • Reduce resolution (512x512 fastest, 768x768 balanced)\")\n",
    "print(\"   • Limit frames (72 frames = 3s, 120 frames = 5s)\")\n",
    "print(\"   • Use bf16 precision when available\")\n",
    "\n",
    "print(\"\\n💾 Memory Optimization:\")\n",
    "print(\"   • Enable CPU offloading (done automatically)\")\n",
    "print(\"   • Use VAE tiling for high resolution\")\n",
    "print(\"   • Call cleanup_memory() between generations\")\n",
    "print(\"   • Close other applications using VRAM\")\n",
    "\n",
    "print(\"\\n🎨 Quality Tips:\")\n",
    "print(\"   • Use descriptive prompts with style keywords\")\n",
    "print(\"   • Add 'cinematic', 'high quality', 'detailed' to prompts\")\n",
    "print(\"   • Experiment with guidance_scale (7.0-9.0 range)\")\n",
    "print(\"   • Use more inference steps for final outputs (50-60)\")\n",
    "\n",
    "# Run diagnostic\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "diagnose_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 12. Conclusion and Next Steps\n",
    "\n",
    "🎉 **Congratulations!** You've successfully set up and used WAN 2.1 for video generation.\n",
    "\n",
    "### What You've Learned:\n",
    "- ✅ Environment detection and automatic optimization\n",
    "- ✅ WAN 2.1 model variants (1.3B and 14B)\n",
    "- ✅ Memory management and performance optimization\n",
    "- ✅ Video generation with custom parameters\n",
    "- ✅ Integration with the videogenbook package\n",
    "\n",
    "### Next Steps:\n",
    "1. **Experiment** with different prompts and parameters\n",
    "2. **Try batch generation** for multiple videos\n",
    "3. **Explore other models** in our videogenbook package\n",
    "4. **Check out Chapter 3** for advanced implementation techniques\n",
    "5. **Join our community** for tips and troubleshooting\n",
    "\n",
    "### Resources:\n",
    "- 📚 [Video Generation Book Repository](https://github.com/jenochs/video-generation-book)\n",
    "- 📖 [WAN 2.1 Paper](https://arxiv.org/abs/2412.04889)\n",
    "- 🤗 [Model on HuggingFace](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B-Diffusers)\n",
    "\n",
    "**Happy video generating! 🎬✨**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}